{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGHZlFGwIzP1",
    "outputId": "6cc3342c-dd32-4bea-bc9b-24022e55502f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chi-value is 354.576974822218 \t words sastra and university are collacted\n",
      "t-test is 4.57 \t words sastra and university are collacted\n",
      " chi-value is 28.728320834496788 \t words university and is are collacted\n",
      "t-test is 2.06 \t words university and is are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and located are collacted\n",
      "t-test is 0.96 \t words is and located are not  collacted\n",
      " chi-value is 37.30583126550869 \t words located and in are collacted\n",
      "t-test is 0.97 \t words located and in are not  collacted\n",
      " chi-value is 37.30583126550869 \t words in and tamil are collacted\n",
      "t-test is 0.97 \t words in and tamil are not  collacted\n",
      " chi-value is 496.99999999999994 \t words tamil and nadu. are collacted\n",
      "t-test is 1.0 \t words tamil and nadu. are not  collacted\n",
      " chi-value is 22.1527665317139 \t words students and at are collacted\n",
      "t-test is 0.96 \t words students and at are not  collacted\n",
      " chi-value is 71.6162421630094 \t words at and sastra are collacted\n",
      "t-test is 2.26 \t words at and sastra are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and engage are collacted\n",
      "t-test is 0.95 \t words university and engage are not  collacted\n",
      " chi-value is 37.30583126550869 \t words engage and in are collacted\n",
      "t-test is 0.97 \t words engage and in are not  collacted\n",
      " chi-value is 37.30583126550869 \t words in and rigorous are collacted\n",
      "t-test is 0.97 \t words in and rigorous are not  collacted\n",
      " chi-value is 98.59838709677422 \t words rigorous and academic are collacted\n",
      "t-test is 0.99 \t words rigorous and academic are not  collacted\n",
      " chi-value is 98.59838709677422 \t words academic and pursuits. are collacted\n",
      "t-test is 0.99 \t words academic and pursuits. are not  collacted\n",
      " chi-value is 226.66209451841257 \t words the and university's are collacted\n",
      "t-test is 3.92 \t words the and university's are collacted\n",
      " chi-value is 18.15190756823821 \t words university's and main are collacted\n",
      "t-test is 0.95 \t words university's and main are not  collacted\n",
      " chi-value is 496.99999999999994 \t words main and campus, are collacted\n",
      "t-test is 1.0 \t words main and campus, are not  collacted\n",
      " chi-value is 14.088954056695991 \t words campus, and sastra are collacted\n",
      "t-test is 0.93 \t words campus, and sastra are not  collacted\n",
      " chi-value is 14.088954056695991 \t words sastra and tanjore, are collacted\n",
      "t-test is 0.93 \t words sastra and tanjore, are not  collacted\n",
      " chi-value is 23.89808467741935 \t words tanjore, and is are collacted\n",
      "t-test is 0.96 \t words tanjore, and is are not  collacted\n",
      " chi-value is 47.89272727272727 \t words is and known are collacted\n",
      "t-test is 1.36 \t words is and known are not  collacted\n",
      " chi-value is 122.7439393939394 \t words known and for are collacted\n",
      "t-test is 1.39 \t words known and for are not  collacted\n",
      " chi-value is 78.1406144807764 \t words for and its are collacted\n",
      "t-test is 1.9 \t words for and its are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and scenic are collacted\n",
      "t-test is 0.98 \t words its and scenic are not  collacted\n",
      " chi-value is 496.99999999999994 \t words scenic and beauty. are collacted\n",
      "t-test is 1.0 \t words scenic and beauty. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and offers are collacted\n",
      "t-test is 0.95 \t words university and offers are not  collacted\n",
      " chi-value is 496.99999999999994 \t words offers and undergraduate are collacted\n",
      "t-test is 1.0 \t words offers and undergraduate are not  collacted\n",
      " chi-value is 40.498151881720425 \t words undergraduate and and are collacted\n",
      "t-test is 0.98 \t words undergraduate and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and postgraduate are collacted\n",
      "t-test is 0.98 \t words and and postgraduate are not  collacted\n",
      " chi-value is 496.99999999999994 \t words postgraduate and programs are collacted\n",
      "t-test is 1.0 \t words postgraduate and programs are not  collacted\n",
      " chi-value is 37.30583126550869 \t words programs and in are collacted\n",
      "t-test is 0.97 \t words programs and in are not  collacted\n",
      " chi-value is 17.700041900248507 \t words in and diverse are collacted\n",
      "t-test is 0.95 \t words in and diverse are not  collacted\n",
      " chi-value is 247.99899193548382 \t words diverse and disciplines. are collacted\n",
      "t-test is 1.0 \t words diverse and disciplines. are not  collacted\n",
      " chi-value is 25.76810223661201 \t words sastra and university's are collacted\n",
      "t-test is 2.22 \t words sastra and university's are not  collacted\n",
      " chi-value is 91.49742026266415 \t words university's and commitment are collacted\n",
      "t-test is 2.12 \t words university's and commitment are not  collacted\n",
      " chi-value is 151.8400660569106 \t words commitment and to are collacted\n",
      "t-test is 2.16 \t words commitment and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and quality are collacted\n",
      "t-test is 0.97 \t words to and quality are not  collacted\n",
      " chi-value is 247.99899193548382 \t words quality and education are collacted\n",
      "t-test is 1.0 \t words quality and education are not  collacted\n",
      " chi-value is 10.990190479215636 \t words education and is are collacted\n",
      "t-test is 0.92 \t words education and is are not  collacted\n",
      " chi-value is 71.98451417004048 \t words is and evident are collacted\n",
      "t-test is 1.66 \t words is and evident are not  collacted\n",
      " chi-value is 112.37060105886015 \t words evident and in are collacted\n",
      "t-test is 1.69 \t words evident and in are not  collacted\n",
      " chi-value is 45.55076014270017 \t words in and its are collacted\n",
      "t-test is 1.84 \t words in and its are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and faculty. are collacted\n",
      "t-test is 0.98 \t words its and faculty. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and reputation are collacted\n",
      "t-test is 0.95 \t words university's and reputation are not  collacted\n",
      " chi-value is 61.248235887096776 \t words reputation and for are collacted\n",
      "t-test is 0.98 \t words reputation and for are not  collacted\n",
      " chi-value is 61.248235887096776 \t words for and excellence are collacted\n",
      "t-test is 0.98 \t words for and excellence are not  collacted\n",
      " chi-value is 247.99899193548382 \t words excellence and extends are collacted\n",
      "t-test is 1.0 \t words excellence and extends are not  collacted\n",
      " chi-value is 247.99899193548382 \t words extends and beyond are collacted\n",
      "t-test is 1.0 \t words extends and beyond are not  collacted\n",
      " chi-value is 247.99899193548382 \t words beyond and national are collacted\n",
      "t-test is 1.0 \t words beyond and national are not  collacted\n",
      " chi-value is 247.99899193548382 \t words national and borders. are collacted\n",
      "t-test is 1.0 \t words national and borders. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and emphasizes are collacted\n",
      "t-test is 0.95 \t words university and emphasizes are not  collacted\n",
      " chi-value is 48.79818548387096 \t words emphasizes and a are collacted\n",
      "t-test is 0.98 \t words emphasizes and a are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and holistic are collacted\n",
      "t-test is 0.98 \t words a and holistic are not  collacted\n",
      " chi-value is 247.99899193548382 \t words holistic and approach are collacted\n",
      "t-test is 1.0 \t words holistic and approach are not  collacted\n",
      " chi-value is 60.3679292929293 \t words approach and to are collacted\n",
      "t-test is 1.37 \t words approach and to are not  collacted\n",
      " chi-value is 95.56810253029765 \t words to and student are collacted\n",
      "t-test is 1.92 \t words to and student are not  collacted\n",
      " chi-value is 23.310475312917426 \t words student and development. are collacted\n",
      "t-test is 0.96 \t words student and development. are not  collacted\n",
      " chi-value is 496.99999999999994 \t words many and aspiring are collacted\n",
      "t-test is 1.0 \t words many and aspiring are not  collacted\n",
      " chi-value is 496.99999999999994 \t words aspiring and engineers are collacted\n",
      "t-test is 1.0 \t words aspiring and engineers are not  collacted\n",
      " chi-value is 496.99999999999994 \t words engineers and choose are collacted\n",
      "t-test is 1.0 \t words engineers and choose are not  collacted\n",
      " chi-value is 14.088954056695991 \t words choose and sastra are collacted\n",
      "t-test is 0.93 \t words choose and sastra are not  collacted\n",
      " chi-value is 1.041081763021858 \t words university and for are not collacted\n",
      "t-test is 0.61 \t words university and for are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and stellar are collacted\n",
      "t-test is 0.98 \t words its and stellar are not  collacted\n",
      " chi-value is 496.99999999999994 \t words stellar and engineering are collacted\n",
      "t-test is 1.0 \t words stellar and engineering are not  collacted\n",
      " chi-value is 247.99899193548382 \t words engineering and programs. are collacted\n",
      "t-test is 1.0 \t words engineering and programs. are not  collacted\n",
      " chi-value is 4.807751100196754 \t words the and alumni are collacted\n",
      "t-test is 0.84 \t words the and alumni are not  collacted\n",
      " chi-value is 164.9986559139785 \t words alumni and network are collacted\n",
      "t-test is 0.99 \t words alumni and network are not  collacted\n",
      " chi-value is 54.331541218637994 \t words network and of are collacted\n",
      "t-test is 0.98 \t words network and of are not  collacted\n",
      " chi-value is 0.295612046601306 \t words of and sastra are not collacted\n",
      "t-test is 0.4 \t words of and sastra are not  collacted\n",
      " chi-value is 17.829186888449797 \t words is and a are collacted\n",
      "t-test is 1.5 \t words is and a are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and valuable are collacted\n",
      "t-test is 0.98 \t words a and valuable are not  collacted\n",
      " chi-value is 496.99999999999994 \t words valuable and resource are collacted\n",
      "t-test is 1.0 \t words valuable and resource are not  collacted\n",
      " chi-value is 61.248235887096776 \t words resource and for are collacted\n",
      "t-test is 0.98 \t words resource and for are not  collacted\n",
      " chi-value is 61.248235887096776 \t words for and graduates. are collacted\n",
      "t-test is 0.98 \t words for and graduates. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and vision are collacted\n",
      "t-test is 0.95 \t words university's and vision are not  collacted\n",
      " chi-value is 23.89808467741935 \t words vision and is are collacted\n",
      "t-test is 0.96 \t words vision and is are not  collacted\n",
      " chi-value is 0.21207500588396805 \t words is and to are not collacted\n",
      "t-test is 0.36 \t words is and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and be are collacted\n",
      "t-test is 0.97 \t words to and be are not  collacted\n",
      " chi-value is 48.79818548387096 \t words be and a are collacted\n",
      "t-test is 0.98 \t words be and a are not  collacted\n",
      " chi-value is 23.454651857382864 \t words a and global are collacted\n",
      "t-test is 0.96 \t words a and global are not  collacted\n",
      " chi-value is 247.99899193548382 \t words global and leader are collacted\n",
      "t-test is 1.0 \t words global and leader are not  collacted\n",
      " chi-value is 37.30583126550869 \t words leader and in are collacted\n",
      "t-test is 0.97 \t words leader and in are not  collacted\n",
      " chi-value is 17.700041900248507 \t words in and education are collacted\n",
      "t-test is 0.95 \t words in and education are not  collacted\n",
      " chi-value is 19.298363879343263 \t words education and and are collacted\n",
      "t-test is 0.95 \t words education and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and research. are collacted\n",
      "t-test is 0.98 \t words and and research. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and collaborates are collacted\n",
      "t-test is 0.95 \t words university and collaborates are not  collacted\n",
      " chi-value is 164.9986559139785 \t words collaborates and with are collacted\n",
      "t-test is 0.99 \t words collaborates and with are not  collacted\n",
      " chi-value is 330.663973063973 \t words with and industry are collacted\n",
      "t-test is 1.41 \t words with and industry are not  collacted\n",
      " chi-value is 247.99899193548382 \t words industry and leaders are collacted\n",
      "t-test is 1.0 \t words industry and leaders are not  collacted\n",
      " chi-value is 30.12310987903226 \t words leaders and to are collacted\n",
      "t-test is 0.97 \t words leaders and to are not  collacted\n",
      " chi-value is 14.104641135891134 \t words to and enhance are collacted\n",
      "t-test is 0.94 \t words to and enhance are not  collacted\n",
      " chi-value is 247.99899193548382 \t words enhance and practical are collacted\n",
      "t-test is 1.0 \t words enhance and practical are not  collacted\n",
      " chi-value is 247.99899193548382 \t words practical and learning. are collacted\n",
      "t-test is 1.0 \t words practical and learning. are not  collacted\n",
      " chi-value is 8.117961008088395 \t words the and department are collacted\n",
      "t-test is 0.9 \t words the and department are not  collacted\n",
      " chi-value is 108.88260381593714 \t words department and of are collacted\n",
      "t-test is 1.39 \t words department and of are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and computer are collacted\n",
      "t-test is 0.98 \t words of and computer are not  collacted\n",
      " chi-value is 496.99999999999994 \t words computer and science are collacted\n",
      "t-test is 1.0 \t words computer and science are not  collacted\n",
      " chi-value is 70.14112903225806 \t words science and at are collacted\n",
      "t-test is 0.99 \t words science and at are not  collacted\n",
      " chi-value is 47.89272727272727 \t words is and highly are collacted\n",
      "t-test is 1.36 \t words is and highly are not  collacted\n",
      " chi-value is 247.99899193548382 \t words highly and regarded. are collacted\n",
      "t-test is 1.0 \t words highly and regarded. are not  collacted\n",
      " chi-value is 39.57592592592593 \t words university and has are collacted\n",
      "t-test is 1.35 \t words university and has are not  collacted\n",
      " chi-value is 247.99899193548382 \t words has and been are collacted\n",
      "t-test is 1.0 \t words has and been are not  collacted\n",
      " chi-value is 496.99999999999994 \t words been and consistently are collacted\n",
      "t-test is 1.0 \t words been and consistently are not  collacted\n",
      " chi-value is 496.99999999999994 \t words consistently and ranked are collacted\n",
      "t-test is 1.0 \t words consistently and ranked are not  collacted\n",
      " chi-value is 496.99999999999994 \t words ranked and among are collacted\n",
      "t-test is 1.0 \t words ranked and among are not  collacted\n",
      " chi-value is 18.15190756823821 \t words among and the are collacted\n",
      "t-test is 0.95 \t words among and the are not  collacted\n",
      " chi-value is 18.15190756823821 \t words the and top are collacted\n",
      "t-test is 0.95 \t words the and top are not  collacted\n",
      " chi-value is 496.99999999999994 \t words top and universities are collacted\n",
      "t-test is 1.0 \t words top and universities are not  collacted\n",
      " chi-value is 37.30583126550869 \t words universities and in are collacted\n",
      "t-test is 0.97 \t words universities and in are not  collacted\n",
      " chi-value is 37.30583126550869 \t words in and india. are collacted\n",
      "t-test is 0.97 \t words in and india. are not  collacted\n",
      " chi-value is 3.178632531084372 \t words the and campus are not collacted\n",
      "t-test is 0.79 \t words the and campus are not  collacted\n",
      " chi-value is 123.4984879032258 \t words campus and life are collacted\n",
      "t-test is 0.99 \t words campus and life are not  collacted\n",
      " chi-value is 70.14112903225806 \t words life and at are collacted\n",
      "t-test is 0.99 \t words life and at are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and vibrant are collacted\n",
      "t-test is 0.96 \t words is and vibrant are not  collacted\n",
      " chi-value is 40.498151881720425 \t words vibrant and and are collacted\n",
      "t-test is 0.98 \t words vibrant and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and diverse. are collacted\n",
      "t-test is 0.98 \t words and and diverse. are not  collacted\n",
      " chi-value is 8.784879485916935 \t words to and research are collacted\n",
      "t-test is 0.9 \t words to and research are not  collacted\n",
      " chi-value is 6.713093318281998 \t words research and is are collacted\n",
      "t-test is 0.88 \t words research and is are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and well-equipped are collacted\n",
      "t-test is 0.98 \t words its and well-equipped are not  collacted\n",
      " chi-value is 496.99999999999994 \t words well-equipped and labs. are collacted\n",
      "t-test is 1.0 \t words well-equipped and labs. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and provides are collacted\n",
      "t-test is 0.95 \t words university and provides are not  collacted\n",
      " chi-value is 164.9986559139785 \t words provides and students are collacted\n",
      "t-test is 0.99 \t words provides and students are not  collacted\n",
      " chi-value is 53.88893806196171 \t words students and with are collacted\n",
      "t-test is 0.98 \t words students and with are not  collacted\n",
      " chi-value is 15.01803060407297 \t words with and a are collacted\n",
      "t-test is 0.94 \t words with and a are not  collacted\n",
      " chi-value is 97.79353535353535 \t words a and conducive are collacted\n",
      "t-test is 1.39 \t words a and conducive are not  collacted\n",
      " chi-value is 330.663973063973 \t words conducive and learning are collacted\n",
      "t-test is 1.41 \t words conducive and learning are not  collacted\n",
      " chi-value is 330.663973063973 \t words learning and environment. are collacted\n",
      "t-test is 1.41 \t words learning and environment. are not  collacted\n",
      " chi-value is 4.807751100196754 \t words the and annual are collacted\n",
      "t-test is 0.84 \t words the and annual are not  collacted\n",
      " chi-value is 164.9986559139785 \t words annual and cultural are collacted\n",
      "t-test is 0.99 \t words annual and cultural are not  collacted\n",
      " chi-value is 496.99999999999994 \t words cultural and fest are collacted\n",
      "t-test is 1.0 \t words cultural and fest are not  collacted\n",
      " chi-value is 70.14112903225806 \t words fest and at are collacted\n",
      "t-test is 0.99 \t words fest and at are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and showcases are collacted\n",
      "t-test is 0.95 \t words university and showcases are not  collacted\n",
      " chi-value is 18.15190756823821 \t words showcases and the are collacted\n",
      "t-test is 0.95 \t words showcases and the are not  collacted\n",
      " chi-value is 18.15190756823821 \t words the and talents are collacted\n",
      "t-test is 0.95 \t words the and talents are not  collacted\n",
      " chi-value is 54.331541218637994 \t words talents and of are collacted\n",
      "t-test is 0.98 \t words talents and of are not  collacted\n",
      " chi-value is 2.9421870207625234 \t words of and its are not collacted\n",
      "t-test is 0.78 \t words of and its are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and students. are collacted\n",
      "t-test is 0.98 \t words its and students. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and library are collacted\n",
      "t-test is 0.95 \t words university's and library are not  collacted\n",
      " chi-value is 23.89808467741935 \t words library and is are collacted\n",
      "t-test is 0.96 \t words library and is are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and hub are collacted\n",
      "t-test is 0.98 \t words a and hub are not  collacted\n",
      " chi-value is 54.331541218637994 \t words hub and of are collacted\n",
      "t-test is 0.98 \t words hub and of are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and knowledge are collacted\n",
      "t-test is 0.98 \t words of and knowledge are not  collacted\n",
      " chi-value is 40.498151881720425 \t words knowledge and and are collacted\n",
      "t-test is 0.98 \t words knowledge and and are not  collacted\n",
      " chi-value is 6.629200751543597 \t words and and academic are collacted\n",
      "t-test is 0.88 \t words and and academic are not  collacted\n",
      " chi-value is 98.59838709677422 \t words academic and resources. are collacted\n",
      "t-test is 0.99 \t words academic and resources. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and motto, are collacted\n",
      "t-test is 0.95 \t words university's and motto, are not  collacted\n",
      " chi-value is 496.99999999999994 \t words motto, and \"knowledge are collacted\n",
      "t-test is 1.0 \t words motto, and \"knowledge are not  collacted\n",
      " chi-value is 23.89808467741935 \t words \"knowledge and is are collacted\n",
      "t-test is 0.96 \t words \"knowledge and is are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and power,\" are collacted\n",
      "t-test is 0.96 \t words is and power,\" are not  collacted\n",
      " chi-value is 496.99999999999994 \t words power,\" and reflects are collacted\n",
      "t-test is 1.0 \t words power,\" and reflects are not  collacted\n",
      " chi-value is 40.498151881720425 \t words reflects and its are collacted\n",
      "t-test is 0.98 \t words reflects and its are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and educational are collacted\n",
      "t-test is 0.98 \t words its and educational are not  collacted\n",
      " chi-value is 496.99999999999994 \t words educational and philosophy. are collacted\n",
      "t-test is 1.0 \t words educational and philosophy. are not  collacted\n",
      " chi-value is 39.57592592592593 \t words university and promotes are collacted\n",
      "t-test is 1.35 \t words university and promotes are not  collacted\n",
      " chi-value is 48.399849306068816 \t words promotes and student are collacted\n",
      "t-test is 0.98 \t words promotes and student are not  collacted\n",
      " chi-value is 98.59838709677422 \t words student and involvement are collacted\n",
      "t-test is 0.99 \t words student and involvement are not  collacted\n",
      " chi-value is 37.30583126550869 \t words involvement and in are collacted\n",
      "t-test is 0.97 \t words involvement and in are not  collacted\n",
      " chi-value is 17.700041900248507 \t words in and community are collacted\n",
      "t-test is 0.95 \t words in and community are not  collacted\n",
      " chi-value is 247.99899193548382 \t words community and service. are collacted\n",
      "t-test is 1.0 \t words community and service. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and stands are collacted\n",
      "t-test is 0.95 \t words university and stands are not  collacted\n",
      " chi-value is 496.99999999999994 \t words stands and out are collacted\n",
      "t-test is 1.0 \t words stands and out are not  collacted\n",
      " chi-value is 61.248235887096776 \t words out and for are collacted\n",
      "t-test is 0.98 \t words out and for are not  collacted\n",
      " chi-value is 19.298363879343263 \t words its and emphasis are collacted\n",
      "t-test is 0.95 \t words its and emphasis are not  collacted\n",
      " chi-value is 247.49595959595956 \t words emphasis and on are collacted\n",
      "t-test is 1.4 \t words emphasis and on are not  collacted\n",
      " chi-value is 123.4984879032258 \t words on and innovation are collacted\n",
      "t-test is 0.99 \t words on and innovation are not  collacted\n",
      " chi-value is 40.498151881720425 \t words innovation and and are collacted\n",
      "t-test is 0.98 \t words innovation and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and creativity. are collacted\n",
      "t-test is 0.98 \t words and and creativity. are not  collacted\n",
      " chi-value is 36.37715617715617 \t words university's and leadership are collacted\n",
      "t-test is 1.34 \t words university's and leadership are not  collacted\n",
      " chi-value is 10.990190479215636 \t words leadership and is are collacted\n",
      "t-test is 0.92 \t words leadership and is are not  collacted\n",
      " chi-value is 10.990190479215636 \t words is and dedicated are collacted\n",
      "t-test is 0.92 \t words is and dedicated are not  collacted\n",
      " chi-value is 14.104641135891136 \t words dedicated and to are collacted\n",
      "t-test is 0.94 \t words dedicated and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and fostering are collacted\n",
      "t-test is 0.97 \t words to and fostering are not  collacted\n",
      " chi-value is 98.59838709677422 \t words fostering and academic are collacted\n",
      "t-test is 0.99 \t words fostering and academic are not  collacted\n",
      " chi-value is 98.59838709677422 \t words academic and excellence. are collacted\n",
      "t-test is 0.99 \t words academic and excellence. are not  collacted\n",
      " chi-value is 8.915413561264513 \t words university and encourages are collacted\n",
      "t-test is 0.9 \t words university and encourages are not  collacted\n",
      " chi-value is 81.66500156763315 \t words encourages and students are collacted\n",
      "t-test is 0.99 \t words encourages and students are not  collacted\n",
      " chi-value is 8.784879485916935 \t words students and to are collacted\n",
      "t-test is 0.9 \t words students and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and participate are collacted\n",
      "t-test is 0.97 \t words to and participate are not  collacted\n",
      " chi-value is 37.30583126550869 \t words participate and in are collacted\n",
      "t-test is 0.97 \t words participate and in are not  collacted\n",
      " chi-value is 17.700041900248507 \t words in and international are collacted\n",
      "t-test is 0.95 \t words in and international are not  collacted\n",
      " chi-value is 247.99899193548382 \t words international and exchange are collacted\n",
      "t-test is 1.0 \t words international and exchange are not  collacted\n",
      " chi-value is 247.99899193548382 \t words exchange and programs. are collacted\n",
      "t-test is 1.0 \t words exchange and programs. are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and business are collacted\n",
      "t-test is 0.98 \t words of and business are not  collacted\n",
      " chi-value is 496.99999999999994 \t words business and administration are collacted\n",
      "t-test is 1.0 \t words business and administration are not  collacted\n",
      " chi-value is 70.14112903225806 \t words administration and at are collacted\n",
      "t-test is 0.99 \t words administration and at are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and renowned. are collacted\n",
      "t-test is 0.96 \t words is and renowned. are not  collacted\n",
      " chi-value is 39.57592592592593 \t words university and actively are collacted\n",
      "t-test is 1.35 \t words university and actively are not  collacted\n",
      " chi-value is 496.9999999999999 \t words actively and engages are collacted\n",
      "t-test is 1.41 \t words actively and engages are not  collacted\n",
      " chi-value is 17.700041900248507 \t words engages and in are collacted\n",
      "t-test is 0.95 \t words engages and in are not  collacted\n",
      " chi-value is 37.30583126550869 \t words in and corporate are collacted\n",
      "t-test is 0.97 \t words in and corporate are not  collacted\n",
      " chi-value is 496.99999999999994 \t words corporate and collaborations are collacted\n",
      "t-test is 1.0 \t words corporate and collaborations are not  collacted\n",
      " chi-value is 61.248235887096776 \t words collaborations and for are collacted\n",
      "t-test is 0.98 \t words collaborations and for are not  collacted\n",
      " chi-value is 29.69015745388445 \t words for and skill are collacted\n",
      "t-test is 0.97 \t words for and skill are not  collacted\n",
      " chi-value is 60.87402268117278 \t words skill and development. are collacted\n",
      "t-test is 0.98 \t words skill and development. are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and social are collacted\n",
      "t-test is 0.97 \t words to and social are not  collacted\n",
      " chi-value is 496.99999999999994 \t words social and responsibility are collacted\n",
      "t-test is 1.0 \t words social and responsibility are not  collacted\n",
      " chi-value is 23.89808467741935 \t words responsibility and is are collacted\n",
      "t-test is 0.96 \t words responsibility and is are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and commendable. are collacted\n",
      "t-test is 0.96 \t words is and commendable. are not  collacted\n",
      " chi-value is 16.301787825573044 \t words university's and campus are collacted\n",
      "t-test is 1.27 \t words university's and campus are not  collacted\n",
      " chi-value is 123.4984879032258 \t words campus and infrastructure are collacted\n",
      "t-test is 0.99 \t words campus and infrastructure are not  collacted\n",
      " chi-value is 496.99999999999994 \t words infrastructure and facilitates are collacted\n",
      "t-test is 1.0 \t words infrastructure and facilitates are not  collacted\n",
      " chi-value is 48.79818548387096 \t words facilitates and a are collacted\n",
      "t-test is 0.98 \t words facilitates and a are not  collacted\n",
      " chi-value is 164.9986559139785 \t words learning and atmosphere. are collacted\n",
      "t-test is 0.99 \t words learning and atmosphere. are not  collacted\n",
      " chi-value is 36.37715617715617 \t words university's and admission are collacted\n",
      "t-test is 1.34 \t words university's and admission are not  collacted\n",
      " chi-value is 247.99899193548382 \t words admission and process are collacted\n",
      "t-test is 1.0 \t words admission and process are not  collacted\n",
      " chi-value is 23.89808467741935 \t words process and is are collacted\n",
      "t-test is 0.96 \t words process and is are not  collacted\n",
      " chi-value is 247.99899193548382 \t words highly and competitive. are collacted\n",
      "t-test is 1.0 \t words highly and competitive. are not  collacted\n",
      " chi-value is 23.454651857382864 \t words has and a are collacted\n",
      "t-test is 0.96 \t words has and a are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and comprehensive are collacted\n",
      "t-test is 0.98 \t words a and comprehensive are not  collacted\n",
      " chi-value is 247.99899193548382 \t words comprehensive and approach are collacted\n",
      "t-test is 1.0 \t words comprehensive and approach are not  collacted\n",
      " chi-value is 98.59838709677422 \t words student and career are collacted\n",
      "t-test is 0.99 \t words student and career are not  collacted\n",
      " chi-value is 123.4984879032258 \t words career and development. are collacted\n",
      "t-test is 0.99 \t words career and development. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words the and faculty are collacted\n",
      "t-test is 0.95 \t words the and faculty are not  collacted\n",
      " chi-value is 70.14112903225806 \t words faculty and at are collacted\n",
      "t-test is 0.99 \t words faculty and at are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and composed are collacted\n",
      "t-test is 0.96 \t words is and composed are not  collacted\n",
      " chi-value is 54.331541218637994 \t words composed and of are collacted\n",
      "t-test is 0.98 \t words composed and of are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and experienced are collacted\n",
      "t-test is 0.98 \t words of and experienced are not  collacted\n",
      " chi-value is 40.498151881720425 \t words experienced and and are collacted\n",
      "t-test is 0.98 \t words experienced and and are not  collacted\n",
      " chi-value is 19.298363879343263 \t words and and dedicated are collacted\n",
      "t-test is 0.95 \t words and and dedicated are not  collacted\n",
      " chi-value is 247.99899193548382 \t words dedicated and educators. are collacted\n",
      "t-test is 1.0 \t words dedicated and educators. are not  collacted\n",
      " chi-value is 4.807751100196754 \t words university's and alumni are collacted\n",
      "t-test is 0.84 \t words university's and alumni are not  collacted\n",
      " chi-value is 164.9986559139785 \t words alumni and make are collacted\n",
      "t-test is 0.99 \t words alumni and make are not  collacted\n",
      " chi-value is 496.99999999999994 \t words make and significant are collacted\n",
      "t-test is 1.0 \t words make and significant are not  collacted\n",
      " chi-value is 496.99999999999994 \t words significant and contributions are collacted\n",
      "t-test is 1.0 \t words significant and contributions are not  collacted\n",
      " chi-value is 30.12310987903226 \t words contributions and to are collacted\n",
      "t-test is 0.97 \t words contributions and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and various are collacted\n",
      "t-test is 0.97 \t words to and various are not  collacted\n",
      " chi-value is 496.99999999999994 \t words various and industries. are collacted\n",
      "t-test is 1.0 \t words various and industries. are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and sustainable are collacted\n",
      "t-test is 0.97 \t words to and sustainable are not  collacted\n",
      " chi-value is 496.99999999999994 \t words sustainable and practices are collacted\n",
      "t-test is 1.0 \t words sustainable and practices are not  collacted\n",
      " chi-value is 23.89808467741935 \t words practices and is are collacted\n",
      "t-test is 0.96 \t words practices and is are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and reflected are collacted\n",
      "t-test is 0.96 \t words is and reflected are not  collacted\n",
      " chi-value is 37.30583126550869 \t words reflected and in are collacted\n",
      "t-test is 0.97 \t words reflected and in are not  collacted\n",
      " chi-value is 8.730099137059728 \t words its and campus are collacted\n",
      "t-test is 0.9 \t words its and campus are not  collacted\n",
      " chi-value is 60.87402268117278 \t words campus and initiatives. are collacted\n",
      "t-test is 0.98 \t words campus and initiatives. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and organizes are collacted\n",
      "t-test is 0.95 \t words university and organizes are not  collacted\n",
      " chi-value is 496.99999999999994 \t words organizes and seminars are collacted\n",
      "t-test is 1.0 \t words organizes and seminars are not  collacted\n",
      " chi-value is 40.498151881720425 \t words seminars and and are collacted\n",
      "t-test is 0.98 \t words seminars and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and conferences are collacted\n",
      "t-test is 0.98 \t words and and conferences are not  collacted\n",
      " chi-value is 30.12310987903226 \t words conferences and to are collacted\n",
      "t-test is 0.97 \t words conferences and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and foster are collacted\n",
      "t-test is 0.97 \t words to and foster are not  collacted\n",
      " chi-value is 98.59838709677422 \t words foster and academic are collacted\n",
      "t-test is 0.99 \t words foster and academic are not  collacted\n",
      " chi-value is 98.59838709677422 \t words academic and discourse. are collacted\n",
      "t-test is 0.99 \t words academic and discourse. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and sports are collacted\n",
      "t-test is 0.95 \t words university's and sports are not  collacted\n",
      " chi-value is 496.99999999999994 \t words sports and facilities are collacted\n",
      "t-test is 1.0 \t words sports and facilities are not  collacted\n",
      " chi-value is 496.99999999999994 \t words facilities and cater are collacted\n",
      "t-test is 1.0 \t words facilities and cater are not  collacted\n",
      " chi-value is 30.12310987903226 \t words cater and to are collacted\n",
      "t-test is 0.97 \t words cater and to are not  collacted\n",
      " chi-value is 1.505990284827554 \t words to and a are not collacted\n",
      "t-test is 0.68 \t words to and a are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and wide are collacted\n",
      "t-test is 0.98 \t words a and wide are not  collacted\n",
      " chi-value is 496.99999999999994 \t words wide and range are collacted\n",
      "t-test is 1.0 \t words wide and range are not  collacted\n",
      " chi-value is 54.331541218637994 \t words range and of are collacted\n",
      "t-test is 0.98 \t words range and of are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and athletic are collacted\n",
      "t-test is 0.98 \t words of and athletic are not  collacted\n",
      " chi-value is 496.99999999999994 \t words athletic and interests. are collacted\n",
      "t-test is 1.0 \t words athletic and interests. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and focus are collacted\n",
      "t-test is 0.95 \t words university's and focus are not  collacted\n",
      " chi-value is 123.4984879032258 \t words focus and on are collacted\n",
      "t-test is 0.99 \t words focus and on are not  collacted\n",
      " chi-value is 40.002280578572346 \t words on and research are collacted\n",
      "t-test is 0.98 \t words on and research are not  collacted\n",
      " chi-value is 81.66500156763315 \t words research and extends are collacted\n",
      "t-test is 0.99 \t words research and extends are not  collacted\n",
      " chi-value is 14.104641135891136 \t words extends and to are collacted\n",
      "t-test is 0.94 \t words extends and to are not  collacted\n",
      " chi-value is 30.12310987903226 \t words to and interdisciplinary are collacted\n",
      "t-test is 0.97 \t words to and interdisciplinary are not  collacted\n",
      " chi-value is 496.99999999999994 \t words interdisciplinary and collaborations. are collacted\n",
      "t-test is 1.0 \t words interdisciplinary and collaborations. are not  collacted\n",
      " chi-value is 22.977611241259094 \t words university's and annual are collacted\n",
      "t-test is 1.3 \t words university's and annual are not  collacted\n",
      " chi-value is 164.9986559139785 \t words annual and convocation are collacted\n",
      "t-test is 0.99 \t words annual and convocation are not  collacted\n",
      " chi-value is 496.99999999999994 \t words convocation and ceremony are collacted\n",
      "t-test is 1.0 \t words convocation and ceremony are not  collacted\n",
      " chi-value is 23.89808467741935 \t words ceremony and is are collacted\n",
      "t-test is 0.96 \t words ceremony and is are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and momentous are collacted\n",
      "t-test is 0.98 \t words a and momentous are not  collacted\n",
      " chi-value is 496.99999999999994 \t words momentous and event are collacted\n",
      "t-test is 1.0 \t words momentous and event are not  collacted\n",
      " chi-value is 70.14112903225806 \t words event and at are collacted\n",
      "t-test is 0.99 \t words event and at are not  collacted\n",
      " chi-value is 70.14112903225806 \t words at and sastra. are collacted\n",
      "t-test is 0.99 \t words at and sastra. are not  collacted\n",
      " chi-value is 19.748067876344088 \t words university and prides are collacted\n",
      "t-test is 0.95 \t words university and prides are not  collacted\n",
      " chi-value is 496.99999999999994 \t words prides and itself are collacted\n",
      "t-test is 1.0 \t words prides and itself are not  collacted\n",
      " chi-value is 123.4984879032258 \t words itself and on are collacted\n",
      "t-test is 0.99 \t words itself and on are not  collacted\n",
      " chi-value is 8.730099137059728 \t words on and its are collacted\n",
      "t-test is 0.9 \t words on and its are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and strong are collacted\n",
      "t-test is 0.98 \t words its and strong are not  collacted\n",
      " chi-value is 247.99899193548382 \t words strong and community are collacted\n",
      "t-test is 1.0 \t words strong and community are not  collacted\n",
      " chi-value is 26.225869119243434 \t words community and of are collacted\n",
      "t-test is 0.96 \t words community and of are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and scholars. are collacted\n",
      "t-test is 0.98 \t words of and scholars. are not  collacted\n",
      " chi-value is 123.24798796041217 \t words leadership and encourages are collacted\n",
      "t-test is 0.99 \t words leadership and encourages are not  collacted\n",
      " chi-value is 247.99899193548382 \t words encourages and an are collacted\n",
      "t-test is 1.0 \t words encourages and an are not  collacted\n",
      " chi-value is 496.99999999999994 \t words an and inclusive are collacted\n",
      "t-test is 1.0 \t words an and inclusive are not  collacted\n",
      " chi-value is 40.498151881720425 \t words inclusive and and are collacted\n",
      "t-test is 0.98 \t words inclusive and and are not  collacted\n",
      " chi-value is 19.298363879343263 \t words and and diverse are collacted\n",
      "t-test is 0.95 \t words and and diverse are not  collacted\n",
      " chi-value is 81.66500156763315 \t words diverse and learning are collacted\n",
      "t-test is 0.99 \t words diverse and learning are not  collacted\n",
      " chi-value is 8.117961008088395 \t words university's and emphasis are collacted\n",
      "t-test is 0.9 \t words university's and emphasis are not  collacted\n",
      " chi-value is 123.4984879032258 \t words on and ethics are collacted\n",
      "t-test is 0.99 \t words on and ethics are not  collacted\n",
      " chi-value is 40.498151881720425 \t words ethics and and are collacted\n",
      "t-test is 0.98 \t words ethics and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and values are collacted\n",
      "t-test is 0.98 \t words and and values are not  collacted\n",
      " chi-value is 496.99999999999994 \t words values and sets are collacted\n",
      "t-test is 1.0 \t words values and sets are not  collacted\n",
      " chi-value is 496.99999999999994 \t words sets and it are collacted\n",
      "t-test is 1.0 \t words sets and it are not  collacted\n",
      " chi-value is 496.99999999999994 \t words it and apart. are collacted\n",
      "t-test is 1.0 \t words it and apart. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and strategic are collacted\n",
      "t-test is 0.95 \t words university's and strategic are not  collacted\n",
      " chi-value is 496.99999999999994 \t words strategic and partnerships are collacted\n",
      "t-test is 1.0 \t words strategic and partnerships are not  collacted\n",
      " chi-value is 247.99899193548382 \t words partnerships and enhance are collacted\n",
      "t-test is 1.0 \t words partnerships and enhance are not  collacted\n",
      " chi-value is 19.298363879343263 \t words enhance and its are collacted\n",
      "t-test is 0.95 \t words enhance and its are not  collacted\n",
      " chi-value is 19.298363879343263 \t words its and global are collacted\n",
      "t-test is 0.95 \t words its and global are not  collacted\n",
      " chi-value is 247.99899193548382 \t words global and reach. are collacted\n",
      "t-test is 1.0 \t words global and reach. are not  collacted\n",
      " chi-value is 5.335973483540802 \t words university and alumni are collacted\n",
      "t-test is 0.86 \t words university and alumni are not  collacted\n",
      " chi-value is 164.9986559139785 \t words alumni and excel are collacted\n",
      "t-test is 0.99 \t words alumni and excel are not  collacted\n",
      " chi-value is 37.30583126550869 \t words excel and in are collacted\n",
      "t-test is 0.97 \t words excel and in are not  collacted\n",
      " chi-value is 37.30583126550869 \t words in and both are collacted\n",
      "t-test is 0.97 \t words in and both are not  collacted\n",
      " chi-value is 247.99899193548382 \t words both and national are collacted\n",
      "t-test is 1.0 \t words both and national are not  collacted\n",
      " chi-value is 19.298363879343263 \t words national and and are collacted\n",
      "t-test is 0.95 \t words national and and are not  collacted\n",
      " chi-value is 19.298363879343263 \t words and and international are collacted\n",
      "t-test is 0.95 \t words and and international are not  collacted\n",
      " chi-value is 247.99899193548382 \t words international and arenas. are collacted\n",
      "t-test is 1.0 \t words international and arenas. are not  collacted\n",
      " chi-value is 98.59838709677422 \t words student and welfare are collacted\n",
      "t-test is 0.99 \t words student and welfare are not  collacted\n",
      " chi-value is 23.89808467741935 \t words welfare and is are collacted\n",
      "t-test is 0.96 \t words welfare and is are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and support are collacted\n",
      "t-test is 0.98 \t words its and support are not  collacted\n",
      " chi-value is 496.99999999999994 \t words support and services. are collacted\n",
      "t-test is 1.0 \t words support and services. are not  collacted\n",
      " chi-value is 23.454651857382864 \t words promotes and a are collacted\n",
      "t-test is 0.96 \t words promotes and a are not  collacted\n",
      " chi-value is 48.79818548387096 \t words a and culture are collacted\n",
      "t-test is 0.98 \t words a and culture are not  collacted\n",
      " chi-value is 54.331541218637994 \t words culture and of are collacted\n",
      "t-test is 0.98 \t words culture and of are not  collacted\n",
      " chi-value is 54.331541218637994 \t words of and continuous are collacted\n",
      "t-test is 0.98 \t words of and continuous are not  collacted\n",
      " chi-value is 247.99899193548382 \t words continuous and learning. are collacted\n",
      "t-test is 1.0 \t words continuous and learning. are not  collacted\n",
      " chi-value is 247.99899193548382 \t words admission and criteria are collacted\n",
      "t-test is 1.0 \t words admission and criteria are not  collacted\n",
      " chi-value is 496.99999999999994 \t words criteria and are are collacted\n",
      "t-test is 1.0 \t words criteria and are are not  collacted\n",
      " chi-value is 496.99999999999994 \t words are and transparent are collacted\n",
      "t-test is 1.0 \t words are and transparent are not  collacted\n",
      " chi-value is 40.498151881720425 \t words transparent and and are collacted\n",
      "t-test is 0.98 \t words transparent and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and merit-based. are collacted\n",
      "t-test is 0.98 \t words and and merit-based. are not  collacted\n",
      " chi-value is 81.66500156763315 \t words engages and with are collacted\n",
      "t-test is 0.99 \t words engages and with are not  collacted\n",
      " chi-value is 247.99899193548382 \t words industry and experts are collacted\n",
      "t-test is 1.0 \t words industry and experts are not  collacted\n",
      " chi-value is 61.248235887096776 \t words experts and for are collacted\n",
      "t-test is 0.98 \t words experts and for are not  collacted\n",
      " chi-value is 61.248235887096776 \t words for and curriculum are collacted\n",
      "t-test is 0.98 \t words for and curriculum are not  collacted\n",
      " chi-value is 123.4984879032258 \t words curriculum and development. are collacted\n",
      "t-test is 0.99 \t words curriculum and development. are not  collacted\n",
      " chi-value is 4.807751100196754 \t words university's and research are collacted\n",
      "t-test is 0.84 \t words university's and research are not  collacted\n",
      " chi-value is 164.9986559139785 \t words research and centers are collacted\n",
      "t-test is 0.99 \t words research and centers are not  collacted\n",
      " chi-value is 496.99999999999994 \t words centers and address are collacted\n",
      "t-test is 1.0 \t words centers and address are not  collacted\n",
      " chi-value is 496.99999999999994 \t words address and contemporary are collacted\n",
      "t-test is 1.0 \t words address and contemporary are not  collacted\n",
      " chi-value is 496.99999999999994 \t words contemporary and challenges. are collacted\n",
      "t-test is 1.0 \t words contemporary and challenges. are not  collacted\n",
      " chi-value is 4.593811952236979 \t words campus and is are collacted\n",
      "t-test is 0.84 \t words campus and is are not  collacted\n",
      " chi-value is 40.498151881720425 \t words its and eco-friendly are collacted\n",
      "t-test is 0.98 \t words its and eco-friendly are not  collacted\n",
      " chi-value is 247.99899193548382 \t words eco-friendly and initiatives. are collacted\n",
      "t-test is 1.0 \t words eco-friendly and initiatives. are not  collacted\n",
      " chi-value is 2.2220772603365426 \t words university's and academic are not collacted\n",
      "t-test is 0.74 \t words university's and academic are not  collacted\n",
      " chi-value is 98.59838709677422 \t words academic and calendar are collacted\n",
      "t-test is 0.99 \t words academic and calendar are not  collacted\n",
      " chi-value is 23.89808467741935 \t words calendar and is are collacted\n",
      "t-test is 0.96 \t words calendar and is are not  collacted\n",
      " chi-value is 23.89808467741935 \t words is and well-structured are collacted\n",
      "t-test is 0.96 \t words is and well-structured are not  collacted\n",
      " chi-value is 40.498151881720425 \t words well-structured and and are collacted\n",
      "t-test is 0.98 \t words well-structured and and are not  collacted\n",
      " chi-value is 40.498151881720425 \t words and and organized. are collacted\n",
      "t-test is 0.98 \t words and and organized. are not  collacted\n",
      " chi-value is 164.9986559139785 \t words annual and technical are collacted\n",
      "t-test is 0.99 \t words annual and technical are not  collacted\n",
      " chi-value is 496.99999999999994 \t words technical and symposium are collacted\n",
      "t-test is 1.0 \t words technical and symposium are not  collacted\n",
      " chi-value is 496.99999999999994 \t words symposium and attracts are collacted\n",
      "t-test is 1.0 \t words symposium and attracts are not  collacted\n",
      " chi-value is 496.99999999999994 \t words attracts and participants are collacted\n",
      "t-test is 1.0 \t words attracts and participants are not  collacted\n",
      " chi-value is 496.99999999999994 \t words participants and from are collacted\n",
      "t-test is 1.0 \t words participants and from are not  collacted\n",
      " chi-value is 496.99999999999994 \t words from and across are collacted\n",
      "t-test is 1.0 \t words from and across are not  collacted\n",
      " chi-value is 18.15190756823821 \t words across and the are collacted\n",
      "t-test is 0.95 \t words across and the are not  collacted\n",
      " chi-value is 18.15190756823821 \t words the and country. are collacted\n",
      "t-test is 0.95 \t words the and country. are not  collacted\n",
      " chi-value is 18.15190756823821 \t words university's and efforts are collacted\n",
      "t-test is 0.95 \t words university's and efforts are not  collacted\n",
      " chi-value is 496.99999999999994 \t words efforts and towards are collacted\n",
      "t-test is 1.0 \t words efforts and towards are not  collacted\n",
      " chi-value is 247.99899193548382 \t words towards and skill are collacted\n",
      "t-test is 1.0 \t words towards and skill are not  collacted\n",
      " chi-value is 247.99899193548382 \t words skill and development are collacted\n",
      "t-test is 1.0 \t words skill and development are not  collacted\n",
      " chi-value is 496.99999999999994 \t words development and contribute are collacted\n",
      "t-test is 1.0 \t words development and contribute are not  collacted\n",
      " chi-value is 30.12310987903226 \t words contribute and to are collacted\n",
      "t-test is 0.97 \t words contribute and to are not  collacted\n",
      " chi-value is 98.59838709677422 \t words student and employability. are collacted\n",
      "t-test is 0.99 \t words student and employability. are not  collacted\n"
     ]
    }
   ],
   "source": [
    "#chi-test & t-test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "df=pd.read_excel('SASTRA University.xlsx',header=None)\n",
    "unigram_count=Counter()\n",
    "bigram_count=Counter()\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "for sentence in df[0]:\n",
    "    tokens=simple_tokenize(sentence)\n",
    "    unigram_count.update(tokens)\n",
    "    for i in range(len(tokens)-1):\n",
    "        bigram=(tokens[i],tokens[i+1])\n",
    "        bigram_count[bigram]+=1\n",
    "\n",
    "N=sum(unigram_count.values())\n",
    "\n",
    "def chi_test(w1,w2,c1,c2,c12):\n",
    "    a=c12\n",
    "    b=c1-a\n",
    "    cx=c2-a\n",
    "    r2=N-c1\n",
    "    d=r2-cx\n",
    "    r1=a+b\n",
    "    e=a+cx\n",
    "    f=b+d\n",
    "    e_w1=(e*r1)/N\n",
    "    e_w2=(f*r1)/N\n",
    "    e_w3=(e*r2)/N\n",
    "    e_w4=(f*r2)/N\n",
    "    sum1=(np.square(a-e_w1))/e_w1 if e_w1>0 else 0\n",
    "    sum2=(np.square(b-e_w2))/e_w2 if e_w2>0 else 0\n",
    "    sum3=(np.square(cx-e_w3))/e_w3 if e_w3>0 else 0\n",
    "    sum4=(np.square(d-e_w4))/e_w4 if e_w4>0 else 0\n",
    "    chi_value=sum1+sum2+sum3+sum4\n",
    "    return {'chi-value':chi_value,'word1':w1,'word2':w2}\n",
    "\n",
    "def t_test(w1,w2,c1,c2,c12):\n",
    "    p_w1=c1/N\n",
    "    p_w2=c2/N\n",
    "    exp_mean=p_w1*p_w2\n",
    "    ob_mean=c12/N\n",
    "    diff=ob_mean-exp_mean\n",
    "    s_2=ob_mean\n",
    "    v=math.sqrt(s_2/N) if N>0 else 1\n",
    "    t=diff/v if v>0 else 0\n",
    "    t=round(t,2)\n",
    "    return {'t_value':t,'word1':w1,'word2':w2}\n",
    "\n",
    "for bigram,observed_count in bigram_count.items():\n",
    "    word1,word2=bigram\n",
    "    c1=unigram_count[word1]\n",
    "    c2=unigram_count[word2]\n",
    "    c12=observed_count\n",
    "    chi_result=chi_test(word1,word2,c1,c2,c12)\n",
    "    if chi_result['chi-value']>3.86:\n",
    "        print(f\" chi-value is {chi_result['chi-value']} \\t words {word1} and {word2} are collacted\")\n",
    "    else:\n",
    "         print(f\" chi-value is {chi_result['chi-value']} \\t words {word1} and {word2} are not collacted\")\n",
    "\n",
    "    t_result=t_test(word1,word2,c1,c2,c12)\n",
    "    if t_result['t_value']>2.76:\n",
    "        print(f\"t-test is {t_result['t_value']} \\t words {word1} and {word2} are collacted\")\n",
    "    else:\n",
    "        print(f\"t-test is {t_result['t_value']} \\t words {word1} and {word2} are not  collacted\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6FBqBpyJg3w",
    "outputId": "c4e60599-0e73-4a7f-f647-43846d3a720f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " original tokens ['the', 'children', 'built', 'a', 'dam', 'on', 'the', 'bank', 'of', 'the', 'river', 'using', 'rocks', 'and', 'sticks']\n",
      " tokens after preprocess ['children', 'built', 'dam', 'bank', 'river', 'using', 'rocks', 'sticks']\n",
      " original tokens ['we', 'need', 'to', 'withdraw', 'some', 'cash', 'from', 'the', 'bank', 'for', 'groceries']\n",
      " tokens after preprocess ['need', 'withdraw', 'cash', 'bank', 'groceries']\n",
      " original tokens ['i', 'need', 'to', 'update', 'my', 'contact', 'information', 'with', 'the', 'bank']\n",
      " tokens after preprocess ['need', 'update', 'contact', 'information', 'bank']\n",
      " original tokens ['the', 'bank', 'provides', 'online', 'banking', 'services', 'for', 'convenience']\n",
      " tokens after preprocess ['bank', 'provides', 'online', 'banking', 'services', 'convenience']\n",
      " original tokens ['the', 'beavers', 'constructed', 'a', 'dam', 'along', 'the', 'bank', 'of', 'the', 'river']\n",
      " tokens after preprocess ['beavers', 'constructed', 'dam', 'along', 'bank', 'river']\n",
      " original tokens ['i', 'need', 'to', 'check', 'my', 'transaction', 'history', 'at', 'the', 'bank']\n",
      " tokens after preprocess ['need', 'check', 'transaction', 'history', 'bank']\n",
      " original tokens ['she', 'works', 'as', 'a', 'financial', 'consultant', 'at', 'the', 'bank']\n",
      " tokens after preprocess ['works', 'financial', 'consultant', 'bank']\n",
      "sentence is The children built a dam on the bank of the river using rocks and sticks. guessed sense is River Border\n",
      "sentence is We need to withdraw some cash from the bank for groceries. guessed sense is River Border\n",
      "sentence is I need to update my contact information with the bank.  guessed sense is Financial Institution\n",
      "sentence is The bank provides online banking services for convenience.  guessed sense is Financial Institution\n",
      "sentence is The beavers constructed a dam along the bank of the river. guessed sense is River Border\n",
      "sentence is I need to check my transaction history at the bank.  guessed sense is Financial Institution\n",
      "sentence is She works as a financial consultant at the bank.  guessed sense is Financial Institution\n"
     ]
    }
   ],
   "source": [
    "#word sense disambiguation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data=pd.read_excel('Bank (1).xlsx')\n",
    "data.columns=['Index','Sentence','Sense']\n",
    "last_five_sentences=data['Sentence'].tail(7)\n",
    "stop_words=set(stopwords.words('english'))\n",
    "financial_keywords = ['money', 'loan', 'teller', 'deposit', 'paycheck', 'account', 'financial', 'services', 'transaction','information']\n",
    "river_keywords = ['river', 'water', 'shore', 'picnic', 'eroded']\n",
    "def preprocess(sentence):\n",
    "    tokens=re.findall(r'\\b\\w+\\b',sentence.lower())\n",
    "    print(f\" original tokens {tokens}\")\n",
    "    filtered_tokens=[word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    print(f\" tokens after preprocess {filtered_tokens}\")\n",
    "    return filtered_tokens\n",
    "\n",
    "guesses=[]\n",
    "for sentence in last_five_sentences:\n",
    "    processed_sentence=preprocess(sentence)\n",
    "    if any(word in processed_sentence for word in financial_keywords):\n",
    "        guesses.append(\"Financial Institution\")\n",
    "    else:\n",
    "        guesses.append(\"River Border\")\n",
    "\n",
    "for sentence,guess in zip(last_five_sentences,guesses):\n",
    "    print(f\"sentence is {sentence} guessed sense is {guess}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGx1t_MhJsCQ",
    "outputId": "429ec05f-5251-46ac-fc3e-04f180723f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the noun: room\n",
      "Enter the verb: a\n",
      "Enter the preposition: with\n",
      "Count of room is 5\n",
      "Count of a is 80\n",
      "Count of room and with is 2\n",
      "Count of a and with is 51\n",
      "Lambda value is -0.06454025219471085 and the preposition is attached with the noun.\n"
     ]
    }
   ],
   "source": [
    "#hindle and rooth\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from math import log2\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('Attachment ambiguity WITH.csv', encoding='latin1')\n",
    "\n",
    "# Join the text from the first column and convert to lowercase\n",
    "text = ' '.join(df.iloc[:, 0].astype(str)).lower()\n",
    "\n",
    "# Tokenize the text using regex to find words\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Create unigrams and bigrams using Counter\n",
    "unigram = Counter(tokens)\n",
    "bigram = Counter(ngrams(tokens, 2))\n",
    "\n",
    "# Get user input for noun, verb, and preposition\n",
    "noun = input(\"Enter the noun: \").lower()\n",
    "verb = input(\"Enter the verb: \").lower()\n",
    "pre = input(\"Enter the preposition: \").lower()\n",
    "\n",
    "# Get counts for the noun, verb, and the bigrams (noun-preposition, verb-preposition)\n",
    "noun_count = unigram[noun]\n",
    "verb_count = unigram[verb]\n",
    "noun_pre_count = bigram[(noun, pre)]\n",
    "verb_pre_count = bigram[(pre,verb)]\n",
    "\n",
    "# Print counts\n",
    "print(f\"Count of {noun} is {noun_count}\")\n",
    "print(f\"Count of {verb} is {verb_count}\")\n",
    "print(f\"Count of {noun} and {pre} is {noun_pre_count}\")\n",
    "print(f\"Count of {verb} and {pre} is {verb_pre_count}\")\n",
    "\n",
    "# Function to calculate the lambda value\n",
    "def cal(noun_count, verb_count, noun_pre_count, verb_pre_count):\n",
    "    if verb_count == 0 or noun_count == 0:\n",
    "        print(\"Invalid counts for noun or verb.\")\n",
    "        return None\n",
    "    if noun_pre_count == 0 and verb_pre_count == 0:\n",
    "        print(\"Zero counts for noun-preposition and verb-preposition.\")\n",
    "        return None\n",
    "    pv1 = verb_pre_count / verb_count if verb_count > 0 else 0\n",
    "    pn1 = noun_pre_count / noun_count if noun_count > 0 else 0\n",
    "    pn0 = 1 - pn1\n",
    "    if pv1 == 0 or pn1 == 0 or pn0 == 0:\n",
    "        print(\"Zero probability.\")\n",
    "        return None\n",
    "    try:\n",
    "        lambd = log2((pv1 * pn0) / pn1)\n",
    "        return lambd\n",
    "    except ValueError as e:\n",
    "        print(\"Error in calculation\")\n",
    "        return None\n",
    "\n",
    "# Calculate the lambda value\n",
    "lamda_value = cal(noun_count, verb_count, noun_pre_count, verb_pre_count)\n",
    "\n",
    "# Check the lambda value and print the result\n",
    "if lamda_value:\n",
    "    if lamda_value > 0:\n",
    "        print(f\"Lambda value is {lamda_value} and the preposition is attached with the verb.\")\n",
    "    else:\n",
    "        print(f\"Lambda value is {lamda_value} and the preposition is attached with the noun.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_z8XHboKTvo",
    "outputId": "e9013fcc-77cb-4dbb-b61a-a82615790874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0315\n",
      "[[0.3     0.021   0.02772]\n",
      " [0.      0.063   0.00378]]\n",
      "0.03149999999999999\n",
      "[[0.105 0.45  1.   ]\n",
      " [0.145 0.35  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "#forward and backward HMM\n",
    "import numpy as np\n",
    "states=['cp','ip']\n",
    "observations=['cola','ice-t','lem']\n",
    "pi=([1,0])\n",
    "obs_seq=[2,1,0]\n",
    "A=np.array([[0.7,0.3],[0.5,0.5]])\n",
    "B=np.array([[0.6,0.1,0.3],[0.1,0.7,0.2]])\n",
    "\n",
    "def forward(A,B,pi,obs_seq):\n",
    "    N=A.shape[0]\n",
    "    T=len(obs_seq)\n",
    "    alpha=np.zeros((N,T))\n",
    "    alpha[:,0]=pi*B[:,obs_seq[0]]\n",
    "    for t in range(1,T):\n",
    "        for j in range(N):\n",
    "            alpha[j,t]=np.dot(alpha[:,t-1],A[:,j])*B[j,obs_seq[t]]\n",
    "    return np.sum(alpha[:,T-1]),alpha\n",
    "\n",
    "def backward(A,B,pi,obs_seq):\n",
    "    N=A.shape[0]\n",
    "    T=len(obs_seq)\n",
    "    beta=np.zeros((N,T))\n",
    "    beta[:,T-1]=1\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(N):\n",
    "            beta[i,t]=np.sum(A[i,:]*B[:,obs_seq[t+1]]*beta[:,t+1])\n",
    "    p_obs=np.sum(pi*B[:,obs_seq[0]]*beta[:,0])\n",
    "    return p_obs,beta\n",
    "\n",
    "p_forward,alpha_matrix=forward(A,B,pi,obs_seq)\n",
    "p_backward,beta_matrix=backward(A,B,pi,obs_seq)\n",
    "\n",
    "print(p_forward)\n",
    "print(alpha_matrix)\n",
    "print(p_backward)\n",
    "print(beta_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKT54MO1K6u4",
    "outputId": "f88bb7fd-b25f-4a1b-a6df-d90680bb3586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0189\n",
      "['cp', 'ip', 'cp']\n",
      "[[0.3     0.021   0.0189 ]\n",
      " [0.      0.063   0.00315]]\n"
     ]
    }
   ],
   "source": [
    "#viterbi HMM\n",
    "import numpy as np\n",
    "states=['cp','ip']\n",
    "observations=['cola','ice-t','lem']\n",
    "pi=([1,0])\n",
    "obs_seq=[2,1,0]\n",
    "A=np.array([[0.7,0.3],[0.5,0.5]])\n",
    "B=np.array([[0.6,0.1,0.3],[0.1,0.7,0.2]])\n",
    "def viterbi(A,B,pi,obs_seq):\n",
    "    N=A.shape[0]\n",
    "    T=len(obs_seq)\n",
    "    delta=np.zeros((N,T))\n",
    "    delta[:,0]=pi*B[:,obs_seq[0]]\n",
    "    for t in range(1,T):\n",
    "        for j in range(N):\n",
    "            delta[j,t]=np.max(delta[:,t-1]*A[:,j])*B[j,obs_seq[t]]\n",
    "    state_seq=np.zeros(T,dtype=int)\n",
    "    state_seq[T-1]=np.argmax(delta[:,T-1])\n",
    "    for t in range(T-2,-1,-1):\n",
    "        state_seq[t]=np.argmax(delta[:,t]*A[:,state_seq[t+1]])\n",
    "        state_seq_names=[states[state_seq[t]] for t in range(T)]\n",
    "        return np.max(delta[:,T-1]),state_seq_names,delta\n",
    "\n",
    "p_max,state_seq,delta_matrix=viterbi(A,B,pi,obs_seq)\n",
    "print(p_max)\n",
    "print(state_seq)\n",
    "print(delta_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr3Sfbs2L8cR",
    "outputId": "ebbd77b1-6d6e-4d2d-f521-6619ca1ad45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1\n",
      "(S (N I) (VP (V watch) (NP (N star) (PP (P with) (N telescope)))))\n",
      "           S                         \n",
      "  _________|____                      \n",
      " |              VP                   \n",
      " |     _________|___                  \n",
      " |    |             NP               \n",
      " |    |     ________|____             \n",
      " |    |    |             PP          \n",
      " |    |    |         ____|______      \n",
      " N    V    N        P           N    \n",
      " |    |    |        |           |     \n",
      " I  watch star     with     telescope\n",
      "\n",
      "probability is 6.75e-05\n",
      "Tree 2\n",
      "(S (NP I) (VP (V watch) (NP (N star) (PP (P with) (N telescope)))))\n",
      "           S                         \n",
      "  _________|____                      \n",
      " |              VP                   \n",
      " |     _________|___                  \n",
      " |    |             NP               \n",
      " |    |     ________|____             \n",
      " |    |    |             PP          \n",
      " |    |    |         ____|______      \n",
      " NP   V    N        P           N    \n",
      " |    |    |        |           |     \n",
      " I  watch star     with     telescope\n",
      "\n",
      "probability is 3.15e-05\n",
      "total Probability of trees:9.9e-05\n"
     ]
    }
   ],
   "source": [
    "#cyk-algorithm\n",
    "import nltk\n",
    "from nltk import PCFG\n",
    "\n",
    "\n",
    "def calc_prob(tree,grammar):\n",
    "    prob=1\n",
    "    for prod in tree.productions():\n",
    "        for rule in grammar.productions():\n",
    "            if rule.lhs()==prod.lhs() and rule.rhs()==prod.rhs():\n",
    "                prob*=rule.prob()\n",
    "                break\n",
    "    return prob\n",
    "\n",
    "grammar = PCFG.fromstring(\"\"\"\n",
    "S -> NP VP [0.7]\n",
    "S -> N VP [0.3]\n",
    "PP -> P N [1.0]\n",
    "NP -> N PP [0.3] | Det N PP [0.6] | 'I' [0.1]\n",
    "VP -> V NP [0.25] | VP PP [0.25] | V PNP [0.25] | VN PP [0.25]\n",
    "Det -> 'an' [0.5] | 'my' [0.2] | 'a' [0.3]\n",
    "N -> 'star' [0.4] | 'telescope' [0.1] | 'I' [0.5]\n",
    "V -> 'watch' [0.3] | 'am' [0.2] | 'an' [0.5]\n",
    "P -> 'with' [0.5] | 'of' [0.5]\n",
    "\"\"\")\n",
    "\n",
    "sent=\"I watch star with telescope\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "parser=nltk.ChartParser(grammar)\n",
    "trees=list(parser.parse(tokens))\n",
    "i=1\n",
    "total_prob=0\n",
    "for tree in trees:\n",
    "    print(f\"Tree {i}\")\n",
    "    i+=1\n",
    "    print(tree)\n",
    "    tree.pretty_print()\n",
    "    prob=calc_prob(tree,grammar)\n",
    "    print(f\"probability is {prob}\")\n",
    "    total_prob+=prob\n",
    "print(f\"total Probability of trees:{total_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F85HSdA6mKEy"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rw8ZYWv0lnEZ",
    "outputId": "69470fb6-a8a6-4218-c1d2-de37475c27ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   education       1.00      1.00      1.00         1\n",
      "        spam       1.00      1.00      1.00         1\n",
      "      sports       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "sentence :i just won a free football match ticket  predicted :spam\n",
      "sentence :machine learning is a fascinating field predicted :education\n",
      "sentence :congratulations you ve won a lottery  predicted :spam\n",
      "sentence :football players train hard for the big game predicted :sports\n",
      "sentence :this is a spam message about a great offer predicted :spam\n",
      "sentence :python programming is a very useful skill predicted :education\n"
     ]
    }
   ],
   "source": [
    "documents=[\n",
    "    \"I love playing football\",\n",
    "    \"Football is a great sport\",\n",
    "    \"I am learning NLP\",\n",
    "    \"NLP is very interesting\",\n",
    "    \"This is a spam message\",\n",
    "    \"You won a free ticket\",\n",
    "    \"Free tickets available now\",\n",
    "    \"Football games are exciting\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"Python programming is useful\",\n",
    "    \"Get rich fast with this scheme\",\n",
    "    \"Congratulations, you won!\"\n",
    "\n",
    "]\n",
    "labels=['sports', 'sports', 'education', 'education', 'spam', 'spam',\n",
    "          'spam', 'sports', 'education', 'education', 'spam', 'spam']\n",
    "\n",
    "def prepoc(sent):\n",
    "  sent=sent.lower()\n",
    "  sent=re.sub(r'\\W',' ',sent)\n",
    "  sent=re.sub(r'\\s+',' ',sent)\n",
    "  return sent\n",
    "\n",
    "doc=[prepoc(sent) for sent in documents]\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(doc).toarray()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,labels,test_size=0.2,random_state=42,stratify=labels)\n",
    "classifier=MultinomialNB(alpha=1.0)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"accuracy is {accuracy}\")\n",
    "classi=classification_report(y_test,y_pred)\n",
    "print(classi)\n",
    "sample_sentences=[\n",
    "    \"I just won a free football match ticket!\",\n",
    "    \"Machine learning is a fascinating field\",\n",
    "    \"Congratulations! You've won a lottery.\",\n",
    "    \"Football players train hard for the big game\",\n",
    "    \"This is a spam message about a great offer\",\n",
    "    \"Python programming is a very useful skill\"\n",
    "]\n",
    "prepoc_sent=[prepoc(sent) for sent in sample_sentences]\n",
    "X_sample=vectorizer.transform(prepoc_sent).toarray()\n",
    "y_pred=classifier.predict(X_sample)\n",
    "i=0\n",
    "for k in prepoc_sent:\n",
    "  print(f\"sentence :{k} predicted :{y_pred[i]}\")\n",
    "  i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dax_rYm6n7tB"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VxAkRsuOwDd8",
    "outputId": "5ac0fbdd-6c67-4fa7-bc1d-73ee042e2b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with bag of words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step - accuracy: 0.0000e+00 - loss: 16.1181\n",
      "BOW MODEL ACCURACY 0.0\n",
      "training with tfidf\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.2500 - loss: 12.0886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.0000e+00 - loss: 16.1181\n",
      "tf idf model accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "data={'text': ['I love programming', 'Python is great', 'I enjoy machine learning',\n",
    "                 'TensorFlow is a powerful tool', 'AI is the future'],\n",
    "        'label': ['positive', 'positive', 'positive', 'positive', 'neutral']}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "label_encoder=LabelEncoder()\n",
    "df['label']=label_encoder.fit_transform(df['label'])\n",
    "X_train,X_test,y_train,y_test=train_test_split(df['text'],df['label'],test_size=0.2,random_state=42)\n",
    "vectorizer_bow=CountVectorizer()\n",
    "X_train_bow=vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow=vectorizer_bow.transform(X_test)\n",
    "vectorizer_tfidf=TfidfVectorizer()\n",
    "X_train_tfidf=vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf=vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "def build_model(input_dim):\n",
    "  model=Sequential()\n",
    "  model.add(Dense(16,activation='relu',input_dim=input_dim))\n",
    "  model.add(Dense(8,activation='relu'))\n",
    "  model.add(Dense(1,activation='relu'))\n",
    "  model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "print(\"training with bag of words\")\n",
    "model_bow=build_model(X_train_bow.shape[1])\n",
    "model_bow.fit(X_train_bow,y_train,epochs=10,batch_size=32,verbose=1)\n",
    "loss,accuracy=model_bow.evaluate(X_test_bow,y_test)\n",
    "print(f\"BOW MODEL ACCURACY {accuracy}\")\n",
    "\n",
    "print(\"training with tfidf\")\n",
    "model_tfidf=build_model(X_train_tfidf.shape[1])\n",
    "model_tfidf.fit(X_train_tfidf,y_train,epochs=10,batch_size=32,verbose=1)\n",
    "loss,accuracy=model_tfidf.evaluate(X_test_tfidf,y_test)\n",
    "print(f\"tf idf model accuracy {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dD2D_QXh5p-z",
    "outputId": "ec34f0e9-e559-4baa-bd3c-15d0c0007f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('playing', 0.17823997139930725), ('basketball', 0.13151343166828156), ('love', 0.07497557997703552), ('i', 0.06797593086957932), ('a', 0.04158330708742142)]\n",
      "0.07497559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "corpus=[\"I love to play football.\",\n",
    "    \"Football is a great sport.\",\n",
    "    \"I enjoy playing basketball.\",\n",
    "    \"Basketball is also a fun game.\",\n",
    "    \"I like watching sports.\",\n",
    "    \"Sports are very entertaining.\"]\n",
    "\n",
    "token=[word_tokenize(sent.lower()) for sent in corpus]\n",
    "model=Word2Vec(sentences=token,vector_size=100,window=5,min_count=1,workers=3)\n",
    "most_similar=model.wv.most_similar('football',topn=5)\n",
    "print(most_similar)\n",
    "similarity=model.wv.similarity('football','love')\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mAe0EOWzTV7A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_Qmi5MODXoc",
    "outputId": "66b5021f-d323-419e-bf9d-b6acc7bd6f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16230, 4) (16230, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 45ms/step - accuracy: 0.0454 - loss: 8.1186\n",
      "Epoch 2/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 38ms/step - accuracy: 0.0512 - loss: 7.1593\n",
      "Epoch 3/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 34ms/step - accuracy: 0.0524 - loss: 6.8989\n",
      "Epoch 4/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 38ms/step - accuracy: 0.0546 - loss: 6.7070\n",
      "Epoch 5/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 34ms/step - accuracy: 0.0648 - loss: 6.5154\n",
      "Epoch 6/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.0719 - loss: 6.3468\n",
      "Epoch 7/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.0790 - loss: 6.1177\n",
      "Epoch 8/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.0812 - loss: 5.9559\n",
      "Epoch 9/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.0872 - loss: 5.7116\n",
      "Epoch 10/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.1014 - loss: 5.4325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f48feffa800>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('ArticlesApril2017.csv')\n",
    "snippets=data['snippet'].dropna().tolist()\n",
    "vocab_size=10000\n",
    "tokenizer=Tokenizer(num_words=vocab_size,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(snippets)\n",
    "sequences=tokenizer.texts_to_sequences(snippets)\n",
    "sequence_length=5\n",
    "input_sequences=[]\n",
    "for seq in sequences:\n",
    "    for i in range(1,len(seq)):\n",
    "        n_gram_sequences=seq[:i+1]\n",
    "        input_sequences.append(n_gram_sequences)\n",
    "\n",
    "input_sequences=pad_sequences(input_sequences,maxlen=sequence_length,padding='pre')\n",
    "X,y=input_sequences[:,:-1],input_sequences[:,-1]\n",
    "y=tf.keras.utils.to_categorical(y,num_classes=vocab_size)\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "model=Sequential([\n",
    "    Embedding(vocab_size,64,input_length=sequence_length-1),\n",
    "    LSTM(100,return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100,activation='relu'),\n",
    "    Dense(vocab_size,activation='softmax')\n",
    "    ])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X,y,epochs=10,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tMiW-M7DdR5",
    "outputId": "5309c603-d246-49d9-b771-34d3a3c553e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A court ruling the president of the president administration has the new york\n"
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text,next_words=10):\n",
    "    for _ in range(next_words):\n",
    "        token_list=tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list=pad_sequences([token_list],maxlen=sequence_length-1,padding='pre')\n",
    "        predicted=model.predict(token_list,verbose=0)\n",
    "        predicted_word_index=np.argmax(predicted,axis=-1)[0]\n",
    "        predicted_word=tokenizer.index_word[predicted_word_index]\n",
    "        if predicted_word=='<OOV>':\n",
    "            continue\n",
    "        seed_text+=' ' + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "seed_text=\"A court ruling\"\n",
    "print(generate_text(seed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmKMos2hQ3Kx",
    "outputId": "18d8abe0-7a8e-4616-fbd6-e7baca60a690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences: ['Wonderful!', 'Definitely!', 'He hung up.', 'I ran home.', 'Who are we?', 'Are you mad?', \"He's my son.\", 'He touched me.', 'My head hurts.', 'I drank coffee.', 'Congratulations!', 'How tall is she?', \"They're animals.\", 'Can you see that?', 'I began to speak.', 'I dislike coffee.', \"I'm hungry again.\", \"I don't accept it.\", \"They're policemen.\", \"What's in the box?\", 'Which is your pen?', 'Are you feeling OK?', 'Help came too late.', 'How are you coming?', 'I do make mistakes.', \"It wasn't my fault.\", \"It's very valuable.\", \"She's not a doctor.\", 'We want to hear it.', 'What will you have?', 'Where was your son?', 'More coffee, please.', 'Tom was very scared.', 'What are you saying?', 'You seem very happy.', \"I don't know who won.\", \"I'll be very careful.\", \"I've thought of that.\", 'She sat on the bench.', 'What else can I lose?', 'Are you two musicians?', \"I don't need anything.\", 'I heard the bell ring.', 'I heard you were sick.', 'I suggest you do that.', \"She's not at home now.\", 'The station is nearby.', 'This made me very sad.', \"I don't speak Japanese.\", \"I don't trust that guy.\", \"Let's take that chance.\", 'Old people walk slowly.', 'She was pale with fear.', 'Show me where it hurts.', 'There was nothing left.', 'This is not a sentence.', 'Was it really that bad?', 'Can I make a phone call?', \"He didn't keep his word.\", 'He made her a bookshelf.', \"It's not all your fault.\", 'She scared the cat away.', 'The work is almost done.', \"What's your room number?\", 'Why do you want to come?', 'He dressed up as a woman.', 'I think about that a lot.', \"I'll let it go this time.\", 'None of them are present.', 'Put the boxes over there.', 'We did that deliberately.', \"We haven't slept in days.\", 'When will you next visit?', 'You have to come at once.', 'Out of sight, out of mind.', 'When did you quit smoking?', 'Where did you put my book?', \"You've arrived very early.\", 'Brush your teeth every day.', 'Did all this really happen?', 'Some of the dogs are alive.', \"That wasn't easy, you know.\", \"The movie's about to start.\", \"We don't understand French.\", 'Are you prepared to do this?', \"I don't know where they are.\", 'The baby is crying for milk.', 'This is a wonderful feature.', 'Tom has finally found a job.', \"We'd better leave her alone.\", 'You must stay where you are.', \"Don't interfere with my work.\", 'How many mangoes do you want?', 'She breathed in the cold air.', 'She was on her way to school.', 'We need money to do anything.', \"I know you're going to say no.\", \"It actually isn't that simple.\", 'Tell me about your daily life.', 'Have you ever been here before?', 'That sounds really interesting.', \"There's no reason to be afraid.\", 'We played basketball yesterday.', 'You have no need to be ashamed.', 'You may go swimming or fishing.', 'You must speak in a loud voice.', \"I'm not allowed to do that here.\", 'My sister is crazy about tennis.', 'Tom and I often chat on the bus.', 'You can leave early if you like.', 'Do you live in this neighborhood?', 'Have you ever had a heart attack?', \"I can't keep you here any longer.\", 'She refuses to say more about it.', 'These pencils are the same color.', 'This is a pretty stupid question.', \"Don't hesitate to ask me for help.\", 'He washed the blood off his hands.', \"I'm really sorry about last night.\", \"I'm usually the one who does that.\", 'She taught music for thirty years.', 'A cat came out from under the desk.', 'How about going out to eat tonight?', 'What he says makes no sense at all.', 'Do you have anything further to say?', \"I don't have time to talk right now.\", \"I don't want to talk to you anymore.\", 'Sitting down all day is bad for you.', 'We need to hire people we can trust.', 'You can eat lunch here in this room.', \"I'm getting off at the next bus stop.\", 'He is teaching Spanish to the children.', 'He was a great poet as well as a doctor.', \"I wouldn't go there today if I were you.\", 'She asked me how many languages I spoke.', \"When you're a father, you'll understand.\", 'Which color do you prefer, blue or green?', 'Do you usually eat breakfast before seven?', 'It was apparent that there was no way out.', 'Do you know the reason why she is so angry?', \"I should've been able to do that by myself.\", 'Please be careful not to let the dog loose.', 'Are you still afraid something might happen?', 'I have to get back home and study for a test.', \"It's high time you left for school, isn't it?\", \"These socks don't stretch when you wash them.\", 'With her heart pounding, she opened the door.', 'Can you tell me where the nearest bus stop is?', \"I've made a mistake, though I didn't intend to.\", 'Some people gain weight when they quit smoking.', \"It's going to take all afternoon and maybe more.\", 'My father often falls asleep while watching television.', 'He has two pencils. One is long and the other one is short.', 'Tom thought he might not be permitted to do that by himself.']\n",
      "Telugu Sentences: ['అద్భుతం!', 'తప్పకుండా', 'అతను పెట్టేసాడు', 'నేను ఇంటికి పరిగెత్తాను', 'మేము ఎవరము ?', 'కోపమొచ్చిందా ?', 'అతను మా అబ్బాయి', 'అతను నన్ను తాకాడు', 'నా తల నొప్పిపుడుతుంది', 'నేను కాఫీ తాగాను', 'అభినందనలు', 'ఆమె ఎంత పొడుగు ?', 'వాళ్ళు మృగాలు', 'నువ్వు అది చూసావా ?', 'నేను మాట్లాడటం మొదలుపెట్టాను', 'నాకు కాఫీ ఇష్టం లేదు', 'నాకు మళ్ళా ఆకలి వేస్తుంది', 'నేను అంగీకరించను', 'వాళ్ళు పోలీసులు.', 'పెట్టె లో ఏముంది?', 'నీ కలం ఏది ?', 'వొంట్లో ఎలా వుంది', 'సహాయం చాలా ఆలస్యముగా వచ్చింది', 'నువ్వు ఎలా వస్తున్నావ్?', 'నేనూ తప్పులు చేస్తాను', 'అది నా తప్పు కాదు.', 'అది చాలా విలువైనది', 'ఆమె వైద్యురాలు కాదు', 'మేము వినాలని అనుకుంటున్నాం', 'నువ్వు ఏమి తీసుకుంటావ్?', 'మీ అబ్బాయి ఎక్కడ వున్నాడు ?', 'దయచేసి ఇంకొంచం కాఫీ ఇవ్వరా.', 'టామ్ చాలా భయపడ్డాడు.', 'నువ్వు ఏం చెప్తున్నావు ?', 'నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్', 'ఎవరు గెలిచారో నాకు తెలియదు', 'నేను చాలా జాగ్రత్తగా ఉంటాను', 'నాకు అది తట్టింది', 'ఆమె బల్ల మీద కూర్చుంది .', 'నేను ఇంకేం కోల్పోతాను ?', 'మీరిద్దరూ సంగీతకారులా?', 'నాకు ఏమీ అవసరంలేదు.', 'నాకు గంట మోగటం వినపడింది', 'నీకు బాగోలేదని విన్నాను', 'నువ్వు అది చెయ్యమని నా సలహా', 'తను ఇప్పుడు ఇంటి దగ్గర లేదు', 'స్టేషన్ దగ్గర్లో వుంది', 'ఇది నన్ను చాలా బాధ పెట్టింది', 'నేను జపనీస్ మాట్లాడను', 'నేను వాడిని నమ్మను', 'మనం ఆ అవకాశం తీసుకుందాం', 'ముసలివాళ్లు నెమ్మదిగా నడుస్తారు', 'ఆమె భయంతో తెల్లబోయింది', 'ఎక్కడ నొప్పి పుడుతుందో నాకు చూపించు', 'ఇంకేమి మిగల్లేదు', 'ఇది ఒక వాక్యం కాదు.', 'అది నిజంగా అంత చెడ్డదా ?', 'నేను ఒక ఫోన్ చేసుకోవచ్చా ?', 'అతను తన మాట నిలబెట్టుకోలేదు', 'అతను ఆమెకి పుస్తకాల అర తయారు చేసాడు', 'ఇదంతా నీ తప్పు కాదు', 'ఆవిడ పిల్లిని భయపెట్టి తరిమేసింది .', 'పని దాదాపుగా అయిపోయింది', 'నీ గది అంకె ఎంత ?', 'మీరు ఎందుకు రావాలనుకుంటున్నారు?', 'అతడు స్త్రీలాగా వస్త్రాలను ధరించాడు.', 'నేను దానిగురించి చాలా ఆలోచిస్తుంటాను.', 'నేను ఈసారికి వదిలేస్తాను', 'వాళ్లెవరు రాలేదు', 'డబ్బాలు అక్కడ పెట్టు', 'మేము అది కావాలని చేశాం', 'మేము రోజుల తరబడి నిద్రపోలేదు', 'మళ్ళీ ఎప్పుడు వస్తావు?', 'నువ్వు ఒక్కసారిగా రావాలి', 'చూపుకి వెలుపల\\u200b, మనసుకి వెలుపల\\u200b', 'పొగ తాగడం ఎప్పుడు ఆపేశావ్?', 'నా పుస్తకం ఎక్కడ పెట్టావ్?', 'నువ్వు చాల త్వరగా వచ్చావు', 'నీ పళ్ళు రోజూ తోముకో', 'ఇదంతా నిజంగా జరిగిందా ?', 'కొన్ని కుక్కలు బ్రతికే ఉన్నాయి', 'అది అంత తేలిక కాదు, తెలుసా', 'చలనచిత్రం మొదలు అవ్వబోతుంది', 'మాకు ఫ్రెంచి అర్ధం కాదు', 'ఇది చెయ్యడానికి సిద్దంగా వున్నావా', 'వాళ్ళు ఎక్కడ ఉన్నారో నాకు తెలియదు', 'పసిపాప పాల కోసం ఏడుస్తుంది', 'ఇది ఒక అద్భుతమైన ఫీచర్', 'టామ్ చివరికి ఉద్యోగం సంపాదించాడు.', 'తనని ఒంటరిగా వదిలెయ్యడం మంచిది', 'నువ్వు ఎక్కడ ఉన్నవో అక్కడే ఉండు', 'నా పని లో అడ్డు రాకు.', 'మీకు ఎన్ని మామిడిపండ్లు కావాలి?', 'ఆమె చల్ల గాలి పీల్చింది .', 'తను స్కూలుకు వెళ్ళే దారిలో వుంది', 'ఏం చెయ్యాలన్నా మనకి డబ్బులు కావాలి', 'నాకు నువ్వు ఒద్దని చెప్తావని తెలుసు.', 'అది అంత సులభం ఏం కాదు', 'నీ రోజువారీ జీవితం గురించి చెప్పు', 'నువ్వు ఇంతకు ముందెప్పుడైనా ఇక్కడకు వచ్చావా?', 'అది చాలా ఆసక్తికరంగా ఉన్నట్లు ఉంది', 'భయపడాల్సిన కారణం ఏమీ లేదు.', 'మేము నిన్న బాస్కెట్ బాల్ ఆడాము', 'నువ్వు సిగ్గు పడాల్సిన అవసరం లేదు', 'నువ్వు ఈత కొట్టడానికో లేక చేపలు పట్టడానికో వెళ్ళొచ్చు', 'నువ్వు గట్టిగా మాట్లాడాలి', 'అది ఇక్కడ చెయ్యడానికి నాకు అనుమతి లేదు', 'మా అక్కకి టెన్నిసంటే పిచ్చి', 'టామ్ మరియు నేను తరచుగా బస్సులో ముచ్చటిస్తుంటాము.', 'నీకు నచ్చితే ముందే వెళ్లిపోవచ్చు', 'నువ్వు ఈ చుట్టుపక్కల నివసిస్తున్నావా?', 'నీకు ఎప్పుడైనా గుండెపోటు వచ్చిందా', 'నిన్ను ఇక్కడ ఇంకా ఎక్కువ సమయం వుంచలేను', 'ఆవిడ ఇంకా ఎక్కువ చెప్పడానికి ఒప్పుకోవట్లేదు', 'ఈ పెన్సిళ్లు ఒకే రంగులో ఉన్నాయి', 'ఇది చాలా తెలివితక్కువ ప్రశ్న', 'నన్ను సహాయం అడగడానికి ఏం సందేహ పడొద్దు', 'అతడు తన చేతులకి అంటిన రక్తాన్ని కడిగేసాడు', 'నిన్న రాత్రికి నన్ను నిజంగా క్షమించు', 'సాదారణంగా అది చేసేది నేనే', 'ఆమె ముప్పై ఏళ్ళ పాటు సంగీతం నేర్పిస్తుంది', 'ఒక పిల్లి డెస్క్ కింద నుండి బయటకు వచ్చింది.', 'ఈ రాత్రి బయటకి వెళ్లి తిందామా ?', 'ఆటను చెప్పేది ఏం అర్ధం పర్థం లేకుండా ఉంది', 'ఆ పైన ఇంకేమైనా చెప్పేది వుందా ?', 'నాకు ఇప్పుడు మాట్లాడేంత సమయం లేదు', 'నేనింక నీతో మాట్లాడదల్చుకోవట్లేదు', 'రోజంతా కూర్చోవడం నీకు మంచిది కాదు', 'మనం నమ్మదగిన వాళ్ళనే పనిలోకి తీసుకోవాలి', 'నువ్వు భోజనం ఇక్కడ ఈ గదిలో తినొచ్చు', 'నేను వచ్చే స్టాపు లో దిగుతాను', 'అతను పిల్లలకి స్పానిష్ నేర్పుతున్నాడు', 'ఆటను ఒక గొప్ప కవే కాదు మంచి వైద్యుడు కూడా', 'నేను నువ్వైతే ఈరోజు అక్కడికి వెళ్ళను', 'నేను ఎన్ని భాషలు మట్లాడుతానని తను అడిగింది', 'నువ్వు నాన్న అయినప్పుడు నీకు అర్ధం అవుతుంది', 'నీకు ఏ రంగు అంటే ఇష్టం , నీలమా లేక పచ్ఛా ?', 'నువ్వు మాములుగా ఏడు కు ముందే టిఫిన చెస్తావా?', 'వేరే దారి లేదని స్పష్టంగా తెలుస్తుంది', 'తను ఎందుకు అంత కోపంగా ఉందొ నీకు తెలుసా ?', 'నా అంతట నేనే చేయగలిగి వుండాల్సింది', 'దయచేసి కుక్కను వదిలిపెట్టకుండ జాగ్రత్త వహించండి.', 'ఏమైనా అవుతుందని ఇంకా భయపడుతున్నావా ?', 'నేను ఇంటికి వెళ్లి పరీక్ష కోసం చదవాలి', 'స్కూలుకి బయలుదేరే సమయం అయ్యింది, కాదా', 'ఈ సాక్స్ ఉతికినప్పుడు సాగవు', 'తన గుండె కొట్టుకుంటూనే ఆమె తలుపు తీసింది', 'దగ్గరలో వున్న బస్ స్టాప్ ఎక్కడో కొంచెం చెప్తావా', 'నేనో పొరపాటు చేసాను, కావాలని కాకపోయినా.', 'పొగత్రాగడం మానేసినప్పుడు కొందరు బరువు పెరుగుతారు', 'మధ్యాహ్నం మొత్తం లేదా ఇంకా ఎక్కువ సమయం పట్టొచ్చు', 'మా నాన్న దురదర్శిని చూస్తూనే నిద్ర పోతాడు', 'తన దగ్గర రెండు పెన్సిళ్ళు వున్నాయి. ఒకటి పొడుగు ఇంకోటి పొట్టి', 'టామ్ తనకు తాను చేయటానికి అనుమతించకపోవచ్చని అనుకున్నాడు.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists for English and Telugu sentences\n",
    "english_sentences = []\n",
    "telugu_sentences = []\n",
    "\n",
    "# Open and read the file\n",
    "with open('MT Eng Tel Dataset.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Split the line by tab\n",
    "        parts = line.split('\\t')\n",
    "\n",
    "        # Check if there are at least 2 parts (English and Telugu sentences)\n",
    "        if len(parts) >= 2:\n",
    "            english_sentences.append(parts[0].strip())  # English sentence\n",
    "            telugu_sentences.append(parts[1].strip())   # Telugu sentence\n",
    "\n",
    "# Print the lists to verify\n",
    "print(\"English Sentences:\", english_sentences)\n",
    "print(\"Telugu Sentences:\", telugu_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdvNpli0Qoge",
    "outputId": "5f23483e-2fdc-4f7c-f1ef-b3b09e3d350d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0104 - loss: 11.0937\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1042 - loss: 5.8106\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3333 - loss: 4.9525\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5625 - loss: 4.5120\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5938 - loss: 4.2658\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5938 - loss: 4.0393\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6042 - loss: 3.8850\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6146 - loss: 3.7781\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6146 - loss: 3.6809\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6250 - loss: 3.5898\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6354 - loss: 3.3847\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6354 - loss: 3.2728\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6562 - loss: 3.1780\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6562 - loss: 3.0937\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6667 - loss: 3.0117\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6771 - loss: 2.9341\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6875 - loss: 2.8511\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6875 - loss: 2.7676\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6979 - loss: 2.6807\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7188 - loss: 2.5129\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7188 - loss: 2.4149\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7188 - loss: 2.1785\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7292 - loss: 2.0369\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7396 - loss: 1.9559\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7500 - loss: 1.8258\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7708 - loss: 1.4294\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7708 - loss: 1.4113\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7708 - loss: 1.3606\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7708 - loss: 1.3523\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7708 - loss: 1.3824\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7604 - loss: 1.5303\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7604 - loss: 1.4627\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 1.4535\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 1.3595\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7604 - loss: 1.3430\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7604 - loss: 1.4052\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7708 - loss: 1.3816\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7812 - loss: 1.2745\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7812 - loss: 1.2517\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7917 - loss: 1.2865\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7812 - loss: 1.1200\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.1122\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.1046\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7812 - loss: 1.1946\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7812 - loss: 1.1344\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7812 - loss: 1.1293\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7708 - loss: 1.1138\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7708 - loss: 1.1681\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7708 - loss: 1.0952\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.0868\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7812 - loss: 1.1358\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7917 - loss: 1.0552\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7917 - loss: 0.9784\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7917 - loss: 0.9696\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7812 - loss: 0.9851\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7812 - loss: 0.9793\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7708 - loss: 0.9092\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7708 - loss: 0.9561\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7812 - loss: 0.9480\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7812 - loss: 1.0146\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7812 - loss: 1.0129\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.0512\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7812 - loss: 1.0498\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7812 - loss: 1.0481\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7708 - loss: 1.1256\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7708 - loss: 1.1632\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7708 - loss: 1.1613\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7708 - loss: 1.1593\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7708 - loss: 1.1570\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7708 - loss: 1.1544\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7812 - loss: 1.1514\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7812 - loss: 1.1870\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7812 - loss: 1.2204\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7812 - loss: 1.2050\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7812 - loss: 1.1301\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7812 - loss: 1.1297\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7812 - loss: 1.1294\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7812 - loss: 1.1291\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7812 - loss: 1.1683\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7812 - loss: 1.1676\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.1666\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7812 - loss: 1.1653\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7812 - loss: 1.1637\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.1618\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7812 - loss: 1.1607\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7812 - loss: 1.1599\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7812 - loss: 1.1591\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7812 - loss: 1.1583\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7812 - loss: 1.1576\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7812 - loss: 1.1568\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7812 - loss: 1.1562\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7812 - loss: 1.1556\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7812 - loss: 1.1550\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7812 - loss: 1.1545\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7812 - loss: 1.1540\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7812 - loss: 1.1535\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7812 - loss: 1.1530\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7812 - loss: 1.1526\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7812 - loss: 1.1521\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7812 - loss: 1.1517\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "[[ 0.23525995 -0.0366017  -0.06955236 -0.05775654 -0.07255374 -0.06674565\n",
      "  -0.07642679 -0.05681538 -0.08328481 -0.07062963 -0.08528087 -0.05313429\n",
      "  -0.05146315 -0.06656933 -0.05410738 -0.06612429 -0.06977814 -0.10347109\n",
      "  -0.0367346  -0.06297384 -0.09305081 -0.05132179 -0.0449089  -0.09537059\n",
      "  -0.06616615 -0.05683861 -0.05834629 -0.07181404 -0.05691556 -0.08656454\n",
      "  -0.08606799 -0.05331139 -0.06973393 -0.07252002 -0.05557611 -0.05086305\n",
      "  -0.07666503 -0.05160753 -0.09895872 -0.05230451 -0.08001189 -0.11042552\n",
      "  -0.11431065 -0.09133092 -0.1396608 ]\n",
      " [ 0.37061787 -0.10759952 -0.20078376 -0.09579571 -0.13124545 -0.12624736\n",
      "  -0.11408973 -0.13365814 -0.12258154 -0.1611     -0.14999327 -0.11394759\n",
      "  -0.1332272  -0.18492639 -0.12151255 -0.1018872  -0.04828523 -0.2176033\n",
      "  -0.08220212 -0.10241563 -0.24451289 -0.12966435 -0.11415216 -0.1663594\n",
      "  -0.10828325 -0.11446439 -0.13773394 -0.1995945  -0.0611048  -0.13738725\n",
      "  -0.17182553 -0.11017406 -0.1019862  -0.14678043 -0.09205949 -0.14891484\n",
      "  -0.10739802 -0.06206107 -0.12329668 -0.11672994 -0.16509703 -0.21615304\n",
      "  -0.20583202 -0.18782514 -0.25292742]\n",
      " [ 0.31630856 -0.23205876 -0.2726184  -0.17479944 -0.19615461 -0.25397313\n",
      "  -0.17638275 -0.26874608 -0.15920027 -0.23339933 -0.29153523 -0.26428545\n",
      "  -0.21576901 -0.31829566 -0.15799245 -0.14346977 -0.07217081 -0.3360086\n",
      "  -0.02878219 -0.22348922 -0.35531336 -0.1923103  -0.21661758 -0.27761754\n",
      "  -0.12288535 -0.17075081 -0.2157366  -0.31784877 -0.09490699 -0.15484689\n",
      "  -0.29484957 -0.24154176 -0.1888187  -0.23185574 -0.13464858 -0.1930442\n",
      "  -0.22147362 -0.0800797  -0.14718816 -0.1789687  -0.25567925 -0.31574744\n",
      "  -0.2941763  -0.30783555 -0.2914669 ]\n",
      " [ 0.08359209 -0.3517397  -0.29090613 -0.23973429 -0.21954031 -0.4754096\n",
      "  -0.20009597 -0.44788456 -0.16308908 -0.3283141  -0.38932452 -0.38587645\n",
      "  -0.24898379 -0.3262234  -0.12869021 -0.14820851 -0.08637413 -0.37682837\n",
      "  -0.10586933 -0.27775824 -0.4097509  -0.21377388 -0.22061832 -0.3704028\n",
      "  -0.11954235 -0.31882083 -0.30416808 -0.43687508 -0.14366964 -0.17831525\n",
      "  -0.4000235  -0.3464988  -0.29241335 -0.31705594 -0.24876465 -0.27345493\n",
      "  -0.3526144  -0.0934841  -0.17389825 -0.27625895 -0.32544938 -0.3084222\n",
      "  -0.33094504 -0.38015488 -0.31150752]\n",
      " [-0.43654227 -0.3073762  -0.20140994 -0.31898773 -0.31038168 -0.85572135\n",
      "  -0.2173989  -0.6403873  -0.18441762 -0.30905065 -0.46922418 -0.58356524\n",
      "  -0.24206045 -0.36159563 -0.24796593 -0.08507121 -0.16426986 -0.56048375\n",
      "  -0.21954617 -0.30345678 -0.35477352 -0.16503651 -0.29747263 -0.6070255\n",
      "  -0.23578846 -0.37619016 -0.30910677 -0.4865563  -0.27376693 -0.32814676\n",
      "  -0.5106836  -0.5863119  -0.39168954 -0.6020672  -0.3232452  -0.32745925\n",
      "  -0.38187367 -0.15007065 -0.17582232 -0.41779396 -0.20945382 -0.267919\n",
      "  -0.30828255 -0.46499273 -0.4384486 ]\n",
      " [-0.34114575 -0.38832343 -0.41530815 -0.39609364 -0.3449345  -1.4177988\n",
      "  -0.43398124 -0.8215331  -0.12172641 -0.5143302  -0.6611218  -0.7580282\n",
      "  -0.19472665 -0.4738642  -0.14961015 -0.18454838 -0.32737586 -0.62587726\n",
      "  -0.326313   -0.32575107 -0.23136368 -0.32217538 -0.26726404 -0.887938\n",
      "  -0.43086678 -0.39040175 -0.3749631  -0.63137585 -0.338001   -0.41952395\n",
      "  -0.7066473  -0.567787   -0.48554772 -0.69185716 -0.38455012 -0.22891928\n",
      "  -0.64688635 -0.25341514 -0.47510135 -0.6191075  -0.4265855  -0.32878494\n",
      "  -0.42769346 -0.43994656 -0.60035104]]\n",
      "Translated Sequence: . తప్పకుండా\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "source_texts = english_sentences[:20]\n",
    "target_texts =telugu_sentences[:20]\n",
    "\n",
    "# Split the data\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(source_texts, target_texts, test_size=0.2, random_state=40)\n",
    "\n",
    "# Create vocabulary from the entire dataset\n",
    "source_vocab = set(word_tokenize(\" \".join(source_texts)))\n",
    "target_vocab = set(word_tokenize(\" \".join(target_texts)))\n",
    "\n",
    "source_vocab_size = len(source_vocab) + 1\n",
    "target_vocab_size = len(target_vocab) + 1\n",
    "\n",
    "# Create mappings from words to integers and vice versa\n",
    "source_word_to_int = {word: idx + 1 for idx, word in enumerate(source_vocab)}\n",
    "target_word_to_int = {word: idx + 1 for idx, word in enumerate(target_vocab)}\n",
    "\n",
    "source_int_to_word = {idx + 1: word for idx, word in enumerate(source_vocab)}\n",
    "target_int_to_word = {idx + 1: word for idx, word in enumerate(target_vocab)}\n",
    "\n",
    "# Encode the training and test sequences\n",
    "source_sequences_train = [[source_word_to_int[word] for word in word_tokenize(text)] for text in Xtrain]\n",
    "target_sequences_train = [[target_word_to_int[word] for word in word_tokenize(text)] for text in Ytrain]\n",
    "\n",
    "source_sequences_test = [[source_word_to_int.get(word, 0) for word in word_tokenize(text)] for text in Xtest]\n",
    "target_sequences_test = [[target_word_to_int.get(word, 0) for word in word_tokenize(text)] for text in Ytest]\n",
    "\n",
    "# Find maximum sequence length\n",
    "max_sequence_length = max(len(seq) for seq in source_sequences_train + source_sequences_test)\n",
    "\n",
    "# Pad sequences\n",
    "source_sequences_train = tf.keras.preprocessing.sequence.pad_sequences(source_sequences_train, maxlen=max_sequence_length)\n",
    "target_sequences_train = tf.keras.preprocessing.sequence.pad_sequences(target_sequences_train, maxlen=max_sequence_length)\n",
    "source_sequences_test = tf.keras.preprocessing.sequence.pad_sequences(source_sequences_test, maxlen=max_sequence_length)\n",
    "target_sequences_test = tf.keras.preprocessing.sequence.pad_sequences(target_sequences_test, maxlen=max_sequence_length)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(source_vocab_size,64,input_length=max_sequence_length),\n",
    "    tf.keras.layers.SimpleRNN(128,return_sequences=True),\n",
    "    tf.keras.layers.Dense(target_vocab_size)\n",
    "])\n",
    "\n",
    "# Compile the model with sparse categorical cross-entropy\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape target sequences to match expected input for sparse categorical loss\n",
    "target_sequences_train = np.array(target_sequences_train).reshape(-1, max_sequence_length)\n",
    "target_sequences_test = np.array(target_sequences_test).reshape(-1, max_sequence_length)\n",
    "\n",
    "# Train the model with validation data\n",
    "model.fit(source_sequences_train, target_sequences_train,epochs=100)\n",
    "\n",
    "# Translation process\n",
    "input_sequence = \"I ran home.\"\n",
    "input_sequence = [source_word_to_int.get(word, 0) for word in input_sequence.split()]\n",
    "input_sequence = tf.keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=max_sequence_length)\n",
    "\n",
    "# Predict the output sequence\n",
    "output_sequence = model.predict(input_sequence)[0]\n",
    "print(output_sequence)\n",
    "output_sequence = [target_int_to_word[np.argmax(word)] for word in output_sequence if np.argmax(word) != 0]\n",
    "\n",
    "print(\"Translated Sequence:\", ' '.join(output_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp-easHXROcN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
