{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "uFhh98BSsz7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMGUqK0VmOFV",
        "outputId": "b13b6cb6-d46e-4126-fa93-c8b3382d8a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', \"'m\", 'learning', 'NLTK']\n",
            "Sentence tokens: ['Hello!', 'How are you doing today?', \"I'm learning NLTK\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "sentence=\"Hello! How are you doing today? I'm learning NLTK\"\n",
        "\n",
        "word_tokens=word_tokenize(sentence)\n",
        "print(\"Word tokens:\",word_tokens)\n",
        "\n",
        "sent_tokens=sent_tokenize(sentence)\n",
        "print(\"Sentence tokens:\",sent_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wordnet\n"
      ],
      "metadata": {
        "id": "Lw0Xp7pts9p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qolycR9BoIoZ",
        "outputId": "24055a3a-1ff6-49f4-83c0-3d6580d5ba0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "synonyms=wordnet.synsets('good')\n",
        "for syn in synonyms:\n",
        "  print(syn.name(),syn.definition())\n",
        "\n",
        "print(\"Examples:\",synonyms[1].examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSDaaFFKoUbS",
        "outputId": "24f2396b-c8d6-431e-a613-2539f7ca27fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good.n.01 benefit\n",
            "good.n.02 moral excellence or admirableness\n",
            "good.n.03 that which is pleasing or valuable or useful\n",
            "commodity.n.01 articles of commerce\n",
            "good.a.01 having desirable or positive qualities especially those suitable for a thing specified\n",
            "full.s.06 having the normally expected amount\n",
            "good.a.03 morally admirable\n",
            "estimable.s.02 deserving of esteem and respect\n",
            "beneficial.s.01 promoting or enhancing well-being\n",
            "good.s.06 agreeable or pleasing\n",
            "good.s.07 of moral excellence\n",
            "adept.s.01 having or showing knowledge and skill and aptitude\n",
            "good.s.09 thorough\n",
            "dear.s.02 with or in a close or intimate relationship\n",
            "dependable.s.04 financially sound\n",
            "good.s.12 most suitable or right for a particular purpose\n",
            "good.s.13 resulting favorably\n",
            "effective.s.04 exerting force or influence\n",
            "good.s.15 capable of pleasing\n",
            "good.s.16 appealing to the mind\n",
            "good.s.17 in excellent physical condition\n",
            "good.s.18 tending to promote physical well-being; beneficial to health\n",
            "good.s.19 not forged\n",
            "good.s.20 not left to spoil\n",
            "good.s.21 generally admired\n",
            "well.r.01 (often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\n",
            "thoroughly.r.02 completely and absolutely (`good' is sometimes used informally for `thoroughly')\n",
            "Examples: ['there is much good to be found in people']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "XjqgvOoWtDzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx75LAbfoxWE",
        "outputId": "a0da3016-56f5-48a3-f550-4cbca50749b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words('english'))\n",
        "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "word_tokens=word_tokenize(text)\n",
        "filtered_words=[word for word in word_tokens if word not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFeKQfQPpPJx",
        "outputId": "100fefd1-645c-497e-d903-3deb653368d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "yFjl4L0CtMYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "words=[\"running\",\"ran\",\"runner\",\"easily\",\"fairly\"]\n",
        "stemmed_words=[ps.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDhNWTTqrs8J",
        "outputId": "84d2a953-7c72-483a-9543-c43747875da1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'ran', 'runner', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "AbQc71QvtQqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize('running',pos='v'))\n",
        "print(lemmatizer.lemmatize('better',pos='a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_0UYhaGsPw1",
        "outputId": "c33ab14d-bec5-4b06-805c-1d6482511517"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named Entity Recognition\n"
      ],
      "metadata": {
        "id": "CLxYQivWtUqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNzp7bZCsr5f",
        "outputId": "b77460b5-c412-49d4-e29e-1ad7a4eb14f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk,pos_tag,word_tokenize\n",
        "text=\"Barack Obama was the 44th President of the United States.\"\n",
        "tokens=word_tokenize(text)\n",
        "pos_tags=pos_tag(tokens)\n",
        "entities=ne_chunk(pos_tags)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAo26SUwtxOY",
        "outputId": "e9971302-ffb7-45c5-f645-8c2748ca8b84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parse Tree"
      ],
      "metadata": {
        "id": "s0WRMgYauVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import CFG\n",
        "grammar=CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Det N\n",
        "  VP -> V PP\n",
        "  PP -> P NP\n",
        "  Det -> 'the'\n",
        "  N -> 'cat'|'mat'\n",
        "  V -> 'sat'\n",
        "  P -> 'on'\n",
        "  \"\"\")\n",
        "parser=nltk.ChartParser(grammar)\n",
        "sentence=\"the cat sat on the mat\".split()\n",
        "for tree in parser.parse(sentence):\n",
        "  tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHgfru6euacs",
        "outputId": "f88bc393-3639-427c-fb38-c7247f8ca5c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             S                     \n",
            "      _______|_______               \n",
            "     |               VP            \n",
            "     |        _______|___           \n",
            "     |       |           PP        \n",
            "     |       |    _______|___       \n",
            "     NP      |   |           NP    \n",
            "  ___|___    |   |        ___|___   \n",
            "Det      N   V   P      Det      N \n",
            " |       |   |   |       |       |  \n",
            "the     cat sat  on     the     mat\n",
            "\n"
          ]
        }
      ]
    }
  ]
}